{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Spektral Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the users of a social network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev convolutions GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) Diffusional Convolutions and many others (see convolutional layers ). You can also find pooling layers , including: MinCut pooling DiffPool Top-K pooling Self-Attention Graph (SAG) pooling Global pooling Global gated attention pooling SortPool Spektral also includes lots of utilities for representing, manipulating, and transforming graphs in your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here . If you want to cite Spektral in your work, refer to our paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi Installation Spektral is compatible with Python 3.6 and above, and is tested on the latest versions of Ubuntu, MacOS, and Windows. Other Linux distros should work as well. The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! pip install spektral New in Spektral 1.0 The 1.0 release of Spektral is an important milestone for the library and brings many new features and improvements. If you have already used Spektral in your projects, the only major change that you need to be aware of is the new datasets API. This is a summary of the new features and changes: The new Graph and Dataset containers standardize how Spektral handles data. This does not impact your models , but makes it easier to use your data in Spektral. The new Loader class hides away all the complexity of creating graph batches. Whether you want to write a custom training loop or use Keras' famous model-dot-fit approach, you only need to worry about the training logic and not the data. The new transforms module implements a wide variety of common operations on graphs, that you can now apply() to your datasets. The new GeneralConv and GeneralGNN classes let you build models that are, well... general. Using state-of-the-art results from recent literature means that you don't need to worry about which layers or architecture to choose. The defaults will work well everywhere. New datasets: QM7 and ModelNet10/40, and a new wrapper for OGB datasets. Major clean-up of the library's structure and dependencies. New examples and tutorials. Contributing Spektral is an open-source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework. The contribution guidelines are available here and a list of feature requests is available here .","title":"Home"},{"location":"#welcome-to-spektral","text":"Spektral is a Python library for graph deep learning, based on the Keras API and TensorFlow 2. The main goal of this project is to provide a simple but flexible framework for creating graph neural networks (GNNs). You can use Spektral for classifying the users of a social network, predicting molecular properties, generating new graphs with GANs, clustering nodes, predicting links, and any other task where data is described by graphs. Spektral implements some of the most popular layers for graph deep learning, including: Graph Convolutional Networks (GCN) Chebyshev convolutions GraphSAGE ARMA convolutions Edge-Conditioned Convolutions (ECC) Graph attention networks (GAT) Approximated Personalized Propagation of Neural Predictions (APPNP) Graph Isomorphism Networks (GIN) Diffusional Convolutions and many others (see convolutional layers ). You can also find pooling layers , including: MinCut pooling DiffPool Top-K pooling Self-Attention Graph (SAG) pooling Global pooling Global gated attention pooling SortPool Spektral also includes lots of utilities for representing, manipulating, and transforming graphs in your graph deep learning projects. See how to get started with Spektral and have a look at the examples for some templates. The source code of the project is available on Github . Read the documentation here . If you want to cite Spektral in your work, refer to our paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi","title":"Welcome to Spektral"},{"location":"#installation","text":"Spektral is compatible with Python 3.6 and above, and is tested on the latest versions of Ubuntu, MacOS, and Windows. Other Linux distros should work as well. The simplest way to install Spektral is from PyPi: pip install spektral To install Spektral from source, run this in a terminal: git clone https://github.com/danielegrattarola/spektral.git cd spektral python setup.py install # Or 'pip install .' To install Spektral on Google Colab : ! pip install spektral","title":"Installation"},{"location":"#new-in-spektral-10","text":"The 1.0 release of Spektral is an important milestone for the library and brings many new features and improvements. If you have already used Spektral in your projects, the only major change that you need to be aware of is the new datasets API. This is a summary of the new features and changes: The new Graph and Dataset containers standardize how Spektral handles data. This does not impact your models , but makes it easier to use your data in Spektral. The new Loader class hides away all the complexity of creating graph batches. Whether you want to write a custom training loop or use Keras' famous model-dot-fit approach, you only need to worry about the training logic and not the data. The new transforms module implements a wide variety of common operations on graphs, that you can now apply() to your datasets. The new GeneralConv and GeneralGNN classes let you build models that are, well... general. Using state-of-the-art results from recent literature means that you don't need to worry about which layers or architecture to choose. The defaults will work well everywhere. New datasets: QM7 and ModelNet10/40, and a new wrapper for OGB datasets. Major clean-up of the library's structure and dependencies. New examples and tutorials.","title":"New in Spektral 1.0"},{"location":"#contributing","text":"Spektral is an open-source project available on Github , and contributions of all types are welcome. Feel free to open a pull request if you have something interesting that you want to add to the framework. The contribution guidelines are available here and a list of feature requests is available here .","title":"Contributing"},{"location":"about/","text":"About Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"about/#about","text":"Spektral is an open-source project freely available on Github under MIT license. The framework is maintained primarily by Daniele Grattarola ( Twitter , website ).","title":"About"},{"location":"creating-dataset/","text":"Creating a Custom Dataset The Dataset class is a new feature of Spektral 1.0 that standardizes how graph datasets are represented in Spektral. In this tutorial, we'll go over a simple example to create a custom dataset with your data. This is also useful if you want to share you datasets publicly or include them as part of Spektral. Essential information You create a dataset by subclassing the spektral.data.Dataset class. The core of datasets is the read() method. This is called at every instantiation of the dataset and must return a list of spektral.data.Graph . It doesn't matter if you read the data from a file or create it on the fly, this is where the dataset is loaded in memory. All datasets have a path property that represents the directory in which the data is stored. This defaults to ~/.spektral/datasets/[ClassName] . You can ignore it if you want. However, each time you instantiate a Dataset it will check whether path exists. If it doesn't, the download() method will be called. You can use download() to define any additional operations that are needed to save your raw data to disk. The method will be called before read() . Both read() and download() are called by the dataset's __init__() method. If you need to override the initialization of your dataset, make sure to call super().__init__() somewhere in your implementation (usually as the last line). Example This is a simple example that shows how to create a custom dataset with five random graphs. We pretend that the data comes from an online source so that we can show how to use download() . We start by overriding the __init__() method to store some custom parameters of the dataset: class MyDataset(Dataset): \"\"\" A dataset of five random graphs. \"\"\" def __init__(self, nodes, feats, **kwargs): self.nodes = nodes self.feats = feats super().__init__(**kwargs) Remember to call super().__init__(**kwargs) as the last line. Then, we simulate downloading the data from the web. Since this method gets called if path does not exist on the system, it makes sense to create the corresponding directory now: def download(self): data = ... # Download from somewhere # Create the directory os.mkdir(self.path) # Write the data to file for i in range(5): x = rand(self.nodes, self.feats) a = randint(0, 2, (self.nodes, self.nodes)) y = randint(0, 2) filename = os.path.join(self.path, f'graph_{i}') np.savez(filename, x=x, a=a, y=y) Finally, we implement the read() method to return a list of Graph objects: def read(self): # We must return a list of Graph objects output = [] for i in range(5): data = np.load(os.path.join(self.path, f'graph_{i}.npz')) output.append( Graph(x=data['x'], a=data['a'], y=data['y']) ) return output We can now instantiate our dataset, which will \"download\" our data and read it into memory: >>> dataset = MyDataset(3, 2) >>> dataset MyDataset(n_graphs=5) We can see that our graphs were saved to file: $ ls ~/.spektral/datasets/MyDataset/ graph_0.npz graph_1.npz graph_2.npz graph_3.npz graph_4.npz so the next time we create MyDataset it will read from the files we have saved. You can now use your custom dataset however you like. Loaders will work, as well as transforms and all other features described in the documentation . Remember that, if you want, you're free to store your data as you prefer. Datasets in Spektral are just there to simplify your workflow, but the library is still designed according to Keras' principle of not getting in your way. If you want to manipulate your data differently, your GNNs will still work. You can also see this script for another example on how to create and use a custom dataset.","title":"Creating a dataset"},{"location":"creating-dataset/#creating-a-custom-dataset","text":"The Dataset class is a new feature of Spektral 1.0 that standardizes how graph datasets are represented in Spektral. In this tutorial, we'll go over a simple example to create a custom dataset with your data. This is also useful if you want to share you datasets publicly or include them as part of Spektral.","title":"Creating a Custom Dataset"},{"location":"creating-dataset/#essential-information","text":"You create a dataset by subclassing the spektral.data.Dataset class. The core of datasets is the read() method. This is called at every instantiation of the dataset and must return a list of spektral.data.Graph . It doesn't matter if you read the data from a file or create it on the fly, this is where the dataset is loaded in memory. All datasets have a path property that represents the directory in which the data is stored. This defaults to ~/.spektral/datasets/[ClassName] . You can ignore it if you want. However, each time you instantiate a Dataset it will check whether path exists. If it doesn't, the download() method will be called. You can use download() to define any additional operations that are needed to save your raw data to disk. The method will be called before read() . Both read() and download() are called by the dataset's __init__() method. If you need to override the initialization of your dataset, make sure to call super().__init__() somewhere in your implementation (usually as the last line).","title":"Essential information"},{"location":"creating-dataset/#example","text":"This is a simple example that shows how to create a custom dataset with five random graphs. We pretend that the data comes from an online source so that we can show how to use download() . We start by overriding the __init__() method to store some custom parameters of the dataset: class MyDataset(Dataset): \"\"\" A dataset of five random graphs. \"\"\" def __init__(self, nodes, feats, **kwargs): self.nodes = nodes self.feats = feats super().__init__(**kwargs) Remember to call super().__init__(**kwargs) as the last line. Then, we simulate downloading the data from the web. Since this method gets called if path does not exist on the system, it makes sense to create the corresponding directory now: def download(self): data = ... # Download from somewhere # Create the directory os.mkdir(self.path) # Write the data to file for i in range(5): x = rand(self.nodes, self.feats) a = randint(0, 2, (self.nodes, self.nodes)) y = randint(0, 2) filename = os.path.join(self.path, f'graph_{i}') np.savez(filename, x=x, a=a, y=y) Finally, we implement the read() method to return a list of Graph objects: def read(self): # We must return a list of Graph objects output = [] for i in range(5): data = np.load(os.path.join(self.path, f'graph_{i}.npz')) output.append( Graph(x=data['x'], a=data['a'], y=data['y']) ) return output We can now instantiate our dataset, which will \"download\" our data and read it into memory: >>> dataset = MyDataset(3, 2) >>> dataset MyDataset(n_graphs=5) We can see that our graphs were saved to file: $ ls ~/.spektral/datasets/MyDataset/ graph_0.npz graph_1.npz graph_2.npz graph_3.npz graph_4.npz so the next time we create MyDataset it will read from the files we have saved. You can now use your custom dataset however you like. Loaders will work, as well as transforms and all other features described in the documentation . Remember that, if you want, you're free to store your data as you prefer. Datasets in Spektral are just there to simplify your workflow, but the library is still designed according to Keras' principle of not getting in your way. If you want to manipulate your data differently, your GNNs will still work. You can also see this script for another example on how to create and use a custom dataset.","title":"Example"},{"location":"creating-layer/","text":"Creating a Message-Passing Layer In this tutorial we go over the MessagePassing interface for creating GNN layers. This is a very flexible class that is based on three main functions: message, aggregate and update. By overriding these methods, you can define the behaviour of your own layers. Essential information The MessagePassing layer can be subclassed to create layers that work in single and disjoint mode using sparse adjacency matrices. This ensures that your layers will work for both node-level and graph-level learning while being very computationally efficient. The functionality of these layers is defined by the message , aggregate and update methods, and is summarized as follows: x_out[i] = update(x[i], aggregate([message(x[j]) for j in neighbours(i)])) The message function computes a transformation of the neighbours of each node. The aggregate function aggregates the messages in a way that is independent of the order in which the messages are processed (like a sum, an average, the maximum, etc). The update function takes the aggregated messages from the neighbours and decides how to transform each node. This message-passing scheme is computed by calling the propagate method of the class, which will return the updated node features ( x_out ). Example In this example we will implement a graph convolutional network ( Kipf & and Welling, 2016 ) using the MessagePassing interface. First, let's add some trainable parameters when creating the layer: class GCN(MessagePassing): def __init__(self, n_out, activation): super().__init__(activation=activation) self.n_out = n_out def build(self, input_shape): n_in = input_shape[0][-1] self.weights = self.add_weight(shape=(n_in, self.n_out)) Note that the Keras keyword activation was passed to the constructor of the superclass. This can be done with any Keras keyword (like regularizers, constraints, etc) and the layer will process them automatically. By default, the call method of MessagePassing layers will only call propagate . We modify it so that it also transforms the node features before starting the propagation: def call(self, inputs): x, a = inputs # Update node features x = tf.matmul(x, self.weights) return self.propagate(x=x, a=a) Then, we implement the message function. The get_sources and get_targets built-in methods can be used to automatically retrieve the node attributes of nodes that are sending (sources) or receiving (targets) a message. For instance, we can use get_targets to access the node features x[j] of all neighbors j . If you need direct access to the edge indices, you can use the index_sources and index_targets attributes. In this case, we only need to get the neighbors' features and return them: def message(self, x): # Get the node features of all neighbors return self.get_sources(x) Then, we define an aggregation function for the messages. We can use a simple average of the nodes: from spektral.layers.ops import scatter_mean def aggregate(self, messages): return scatter_mean(messages, self.index_targets, self.n_nodes) Note : n_nodes is computed dynamically at the start of propagation, exactly like index_targets . Since there are a few common aggregation functions that are often used in the literature, you can also skip the implementation of this method and simply pass a special keyword to the __init__() method of the superclass: def __init__(self): # Equivalent to the above implementation of aggregate super().__init__(aggregate='mean') Finally, we can use the update method to apply the activation function: def update(self, embeddings): return self.activation(embeddings) This is enough to get started with building your own layers in Spektral. Notes An important feature of the MessagePassing class is that any extra keyword argument given to propagate , will be compared to the signatures of message , aggregate and update and forwarded to those functions if a match is found. For example, we can call: propagate(x=x, a=a, extra_tensor=extra_tensor) and define the message function as: def message(self, x, extra_tensor=None): ... # Do something with extra_tensor Finally, we already noted that MessagePassing layers only support single and disjoint mode, and they also require that the adjacency matrix is a SparseTensor. If you need more control on your layers, you can have a look at spektral.layers.Conv for a stripped-down class that performs no checks on the inputs and only implements some essential features like keyword parsing. For example, spektral.layers.GCNConv implements the same GCN layer that we just saw, using the Conv class so that it can provide support for batch and mixed mode, as well as dense adjacency matrices.","title":"Creating a layer"},{"location":"creating-layer/#creating-a-message-passing-layer","text":"In this tutorial we go over the MessagePassing interface for creating GNN layers. This is a very flexible class that is based on three main functions: message, aggregate and update. By overriding these methods, you can define the behaviour of your own layers.","title":"Creating a Message-Passing Layer"},{"location":"creating-layer/#essential-information","text":"The MessagePassing layer can be subclassed to create layers that work in single and disjoint mode using sparse adjacency matrices. This ensures that your layers will work for both node-level and graph-level learning while being very computationally efficient. The functionality of these layers is defined by the message , aggregate and update methods, and is summarized as follows: x_out[i] = update(x[i], aggregate([message(x[j]) for j in neighbours(i)])) The message function computes a transformation of the neighbours of each node. The aggregate function aggregates the messages in a way that is independent of the order in which the messages are processed (like a sum, an average, the maximum, etc). The update function takes the aggregated messages from the neighbours and decides how to transform each node. This message-passing scheme is computed by calling the propagate method of the class, which will return the updated node features ( x_out ).","title":"Essential information"},{"location":"creating-layer/#example","text":"In this example we will implement a graph convolutional network ( Kipf & and Welling, 2016 ) using the MessagePassing interface. First, let's add some trainable parameters when creating the layer: class GCN(MessagePassing): def __init__(self, n_out, activation): super().__init__(activation=activation) self.n_out = n_out def build(self, input_shape): n_in = input_shape[0][-1] self.weights = self.add_weight(shape=(n_in, self.n_out)) Note that the Keras keyword activation was passed to the constructor of the superclass. This can be done with any Keras keyword (like regularizers, constraints, etc) and the layer will process them automatically. By default, the call method of MessagePassing layers will only call propagate . We modify it so that it also transforms the node features before starting the propagation: def call(self, inputs): x, a = inputs # Update node features x = tf.matmul(x, self.weights) return self.propagate(x=x, a=a) Then, we implement the message function. The get_sources and get_targets built-in methods can be used to automatically retrieve the node attributes of nodes that are sending (sources) or receiving (targets) a message. For instance, we can use get_targets to access the node features x[j] of all neighbors j . If you need direct access to the edge indices, you can use the index_sources and index_targets attributes. In this case, we only need to get the neighbors' features and return them: def message(self, x): # Get the node features of all neighbors return self.get_sources(x) Then, we define an aggregation function for the messages. We can use a simple average of the nodes: from spektral.layers.ops import scatter_mean def aggregate(self, messages): return scatter_mean(messages, self.index_targets, self.n_nodes) Note : n_nodes is computed dynamically at the start of propagation, exactly like index_targets . Since there are a few common aggregation functions that are often used in the literature, you can also skip the implementation of this method and simply pass a special keyword to the __init__() method of the superclass: def __init__(self): # Equivalent to the above implementation of aggregate super().__init__(aggregate='mean') Finally, we can use the update method to apply the activation function: def update(self, embeddings): return self.activation(embeddings) This is enough to get started with building your own layers in Spektral.","title":"Example"},{"location":"creating-layer/#notes","text":"An important feature of the MessagePassing class is that any extra keyword argument given to propagate , will be compared to the signatures of message , aggregate and update and forwarded to those functions if a match is found. For example, we can call: propagate(x=x, a=a, extra_tensor=extra_tensor) and define the message function as: def message(self, x, extra_tensor=None): ... # Do something with extra_tensor Finally, we already noted that MessagePassing layers only support single and disjoint mode, and they also require that the adjacency matrix is a SparseTensor. If you need more control on your layers, you can have a look at spektral.layers.Conv for a stripped-down class that performs no checks on the inputs and only implements some essential features like keyword parsing. For example, spektral.layers.GCNConv implements the same GCN layer that we just saw, using the Conv class so that it can provide support for batch and mixed mode, as well as dense adjacency matrices.","title":"Notes"},{"location":"data-modes/","text":"Data modes Creating mini-batches of data can be tricky when the samples have different shapes. In traditional neural networks, we're used to stretching, cropping, or padding our data so that all inputs to our models are standardized. For instance, images of different sizes can be modified so that they fit into a tensor of shape [batch, width, height, channels] . Sequences can be padded so that they have shape [batch, time, channels] . And so on... With graphs, it's a bit different. For instance, it is not that easy to define the meaning of \"cropping\" or \"stretching\" a graph, since these are all transformations that assume a \"spatial closeness\" of the pixels (which we don't have for graphs in general). Also, it's not always the case that we have many graphs in our datasets. Sometimes, we're just interested in classifying the nodes of one big graph. Sometimes, we may have one big graph but many instances of its node features (the classification of images is one such case: one grid, many instances of pixels values). To make Spektral work in all of these cases, and to account for the difficulties in dealing with graphs of different sizes, we introduce the concept of data modes . In Spektral, there are four of them: In single mode , we have only one graph. Node classification tasks are usually in this mode. In disjoint mode , we represent a batch of graphs with their disjoint union. This gives us one big graph, similar to single mode, although with some differences (see below). In batch mode , we zero-pad the graphs so that we can fit them into dense tensors of shape [batch, nodes, ...] . This can be more expensive, but makes it easier to interface with traditional NNs. In mixed mode , we have one adjacency matrix shared by many graphs. We keep the adjacency matrix in single mode (for performance, no need to duplicate it for each graph), and the node attributes in batch mode. In all data modes, our goal is to represent one or more graphs by grouping their respective x , a and e matrices into single tensors X , A , and E . The shapes of these tensors in the different data modes are summarized in the table below. Mode A.shape X.shape E.shape Single [nodes, nodes] [nodes, n_feat] [edges, e_feat] Disjoint [nodes, nodes] [nodes, n_feat] [edges, e_feat] Batch [batch, nodes, nodes] [batch, nodes, n_feat] [batch, nodes, nodes, e_feat] Mixed [nodes, nodes] [batch, nodes, n_feat] [batch, edges, e_feat] In the table above, batch is the batch size, nodes is the number of nodes, edges is the number of edges, n_feat and e_feat are the number of node and edge features respectively. Make sure to read the Getting Started tutorial to understand what these matrices represent for a generic graph. In the following sections we describe the four modes more into detail. In particular, we go over which data Loader to use in each case. Single mode In single mode we have only one graph in which: A is a matrix of shape [nodes, nodes] ; X is a matrix of shape [nodes, n_feat] ; E has shape [edges, e_feat] with one row for each non-zero entry of A , sorted in row-major ordering (see the Getting Started tutorial). A very common benchmark dataset in single mode is the Cora citation network. We can load it with: >>> from spektral.datasets import Cora >>> dataset = Cora() >>> dataset Cora(n_graphs=1) As expected, we have only one graph: >>> dataset[0] Graph(n_nodes=2708, n_node_features=1433, n_edge_features=None, n_labels=7) >>> dataset[0].a.shape (2708, 2708) >>> dataset[0].x.shape (2708, 1433) When training a GNN in single mode, we can use a SingleLoader that will extract the characteristic matrices from the graph and return a tf.data.Dataset to feed to our model: >>> from spektral.data import SingleLoader >>> loader = SingleLoader(dataset) >>> loader.load() <RepeatDataset shapes: (((2708, 1433), (2708, 2708)), (2708, 7)), types: ((tf.float32, tf.int64), tf.int32)> Disjoint mode In disjoint mode we represent a set of graphs as a single graph, their \"disjoint union\", where: A is a sparse block diagonal matrix where each block is the adjacency matrix a_i of the i-th graph. X is obtained by stacking the node attributes x_i ; E is also obtained by stacking the edges e_i . The shapes of the three matrices are the same as single mode, but nodes is the cumulative number of all the nodes in the set of graphs. Similarly, the edge features are represented in sparse COOrdinate format and row-major ordering relative to each graph (see the Getting Started tutorial), and edges indicates the cumulative number of edges of the disjoint union. To keep track of the different graphs in the disjoint union, we use an additional array of zero-based indices I that identify which nodes belong to which graph. For instance: if node 8 belongs to the third graph, we will have I[8] == 2 . In the example above, color blue represents 0, green is 1, and orange is 2. In convolutional layers, disjoint mode is indistinguishable from single mode because it is not possible to exchange messages between the disjoint components of the graph, so I is not needed to compute the output. Pooling layers, on the other hand, require I to know which nodes can be pooled together. Let's load a dataset with many small graphs and have a look at the first three: >>> from spektral.datasets import TUDataset >>> dataset = TUDataset('PROTEINS') Successfully loaded PROTEINS. >>> dataset = dataset[:3] >>> dataset[0] Graph(n_nodes=42, n_node_features=4, n_edge_features=None, n_labels=2) >>> dataset[1] Graph(n_nodes=27, n_node_features=4, n_edge_features=None, n_labels=2) >>> dataset[2] Graph(n_nodes=10, n_node_features=4, n_edge_features=None, n_labels=2) To create batches in disjoint mode, we can use a DisjointLoader : >>> from spektral.data import DisjointLoader >>> loader = DisjointLoader(dataset, batch_size=3) Since Loaders are effectively generators, we can inspect the first batch by calling __next__() : >>> batch = loader.__next__() >>> inputs, target = batch >>> x, a, i = inputs >>> x.shape (79, 4) # 79 == 42 + 27 + 10 >>> a.shape (79, 79) >>> i.shape (79, ) Note that, since we don't have edge attributes in our dataset, the loader did not create the E matrix. Batch mode In batch mode, graphs are zero-padded so that they fit into tensors of shape [batch, N, ...] . Due to the general lack of support for sparse higher-order tensors both in Scipy and TensorFlow, X , A , and E will be dense tensors: A has shape [batch, nodes, nodes] ; X has shape [batch, nodes, n_feat] ; E has shape [batch, nodes, nodes, e_feat] (note that this is now the dense/ np.array format, in which the attributes of non-existing edges are all zeros). If the graphs have a variable number of nodes, nodes will be the size of the biggest graph in the batch. If you don't want to zero-pad the graphs or work with dense inputs, it is better to use disjoint mode instead. However, note that some pooling layers like DiffPool and MinCutPool will only work in batch mode. Let's re-use the dataset from the example above. We can use a BatchLoader as follows: >>> from spektral.data import BatchLoader >>> loader = BatchLoader(dataset, batch_size=3) >>> inputs, target = loader.__next__() >>> inputs[0].shape (3, 42, 4) >>> inputs[1].shape (3, 42, 42) In this case, the loader only created two inputs because we don't need the indices I . Also note that the batch was padded so that all graphs have 42 nodes, which is the size of the biggest graph out of the three. The BatchLoader zero-pads each batch independently of the others, so that we don't waste memory. If you want to remove the overhead of padding each batch, you can use a PackedBatchLoader which will pre-pad all graphs before yielding the batches. Of course, this means that all graphs will have the same number of nodes as the biggest graph in the dataset (and not just in the batch). Mixed mode In mixed mode we have a single graph that acts as the support for different node attributes (also sometimes called \"graph signals\"). In this case we have that: A is a matrix of shape [nodes, nodes] ; X is a tensor in batch mode, of shape [batch, nodes, n_feat] ; E has shape [batch, edges, e_feat] , where again we are representing each edge feature matrix E[b] , for b = 0 , ..., batch - 1 , in sparse format. Note that, since nodes and edges are the same for all graphs, we have stacked each x_i and e_i in higher-order tensors, similar to batch mode. An example of a mixed mode dataset is the MNIST random grid ( Defferrard et al., 2016 ): >>> from spektral.datasets import MNIST >>> dataset = MNIST() >>> dataset MNIST(n_graphs=70000) Mixed-mode datasets have a special a attribute that stores the adjacency matrix, while the proper graphs that make up the dataset only have node/edge features: >>>dataset.a <784x784 sparse matrix of type '<class 'numpy.float64'>' with 6396 stored elements in Compressed Sparse Row format> >>> dataset[0] Graph(n_nodes=784, n_node_features=1, n_edge_features=None, n_labels=1) >>>dataset[0].a # None We can use a MixedLoader to deal with sharing the adjacency matrix between the graphs in our dataset: >>> from spektral.data import MixedLoader >>> loader = MixedLoader(dataset, batch_size=3) >>> inputs, target = loader.__next__() >>> inputs[0].shape (3, 784, 1) >>> inputs[1].shape # Only one adjacency matrix (784, 784) Mixed mode requires a bit more work than the other three modes. In particular, it is not possible to use loader.load() to train a model in this mode. Have a look at this example to see how to train a GNN in mixed mode.","title":"Data modes"},{"location":"data-modes/#data-modes","text":"Creating mini-batches of data can be tricky when the samples have different shapes. In traditional neural networks, we're used to stretching, cropping, or padding our data so that all inputs to our models are standardized. For instance, images of different sizes can be modified so that they fit into a tensor of shape [batch, width, height, channels] . Sequences can be padded so that they have shape [batch, time, channels] . And so on... With graphs, it's a bit different. For instance, it is not that easy to define the meaning of \"cropping\" or \"stretching\" a graph, since these are all transformations that assume a \"spatial closeness\" of the pixels (which we don't have for graphs in general). Also, it's not always the case that we have many graphs in our datasets. Sometimes, we're just interested in classifying the nodes of one big graph. Sometimes, we may have one big graph but many instances of its node features (the classification of images is one such case: one grid, many instances of pixels values). To make Spektral work in all of these cases, and to account for the difficulties in dealing with graphs of different sizes, we introduce the concept of data modes . In Spektral, there are four of them: In single mode , we have only one graph. Node classification tasks are usually in this mode. In disjoint mode , we represent a batch of graphs with their disjoint union. This gives us one big graph, similar to single mode, although with some differences (see below). In batch mode , we zero-pad the graphs so that we can fit them into dense tensors of shape [batch, nodes, ...] . This can be more expensive, but makes it easier to interface with traditional NNs. In mixed mode , we have one adjacency matrix shared by many graphs. We keep the adjacency matrix in single mode (for performance, no need to duplicate it for each graph), and the node attributes in batch mode. In all data modes, our goal is to represent one or more graphs by grouping their respective x , a and e matrices into single tensors X , A , and E . The shapes of these tensors in the different data modes are summarized in the table below. Mode A.shape X.shape E.shape Single [nodes, nodes] [nodes, n_feat] [edges, e_feat] Disjoint [nodes, nodes] [nodes, n_feat] [edges, e_feat] Batch [batch, nodes, nodes] [batch, nodes, n_feat] [batch, nodes, nodes, e_feat] Mixed [nodes, nodes] [batch, nodes, n_feat] [batch, edges, e_feat] In the table above, batch is the batch size, nodes is the number of nodes, edges is the number of edges, n_feat and e_feat are the number of node and edge features respectively. Make sure to read the Getting Started tutorial to understand what these matrices represent for a generic graph. In the following sections we describe the four modes more into detail. In particular, we go over which data Loader to use in each case.","title":"Data modes"},{"location":"data-modes/#single-mode","text":"In single mode we have only one graph in which: A is a matrix of shape [nodes, nodes] ; X is a matrix of shape [nodes, n_feat] ; E has shape [edges, e_feat] with one row for each non-zero entry of A , sorted in row-major ordering (see the Getting Started tutorial). A very common benchmark dataset in single mode is the Cora citation network. We can load it with: >>> from spektral.datasets import Cora >>> dataset = Cora() >>> dataset Cora(n_graphs=1) As expected, we have only one graph: >>> dataset[0] Graph(n_nodes=2708, n_node_features=1433, n_edge_features=None, n_labels=7) >>> dataset[0].a.shape (2708, 2708) >>> dataset[0].x.shape (2708, 1433) When training a GNN in single mode, we can use a SingleLoader that will extract the characteristic matrices from the graph and return a tf.data.Dataset to feed to our model: >>> from spektral.data import SingleLoader >>> loader = SingleLoader(dataset) >>> loader.load() <RepeatDataset shapes: (((2708, 1433), (2708, 2708)), (2708, 7)), types: ((tf.float32, tf.int64), tf.int32)>","title":"Single mode"},{"location":"data-modes/#disjoint-mode","text":"In disjoint mode we represent a set of graphs as a single graph, their \"disjoint union\", where: A is a sparse block diagonal matrix where each block is the adjacency matrix a_i of the i-th graph. X is obtained by stacking the node attributes x_i ; E is also obtained by stacking the edges e_i . The shapes of the three matrices are the same as single mode, but nodes is the cumulative number of all the nodes in the set of graphs. Similarly, the edge features are represented in sparse COOrdinate format and row-major ordering relative to each graph (see the Getting Started tutorial), and edges indicates the cumulative number of edges of the disjoint union. To keep track of the different graphs in the disjoint union, we use an additional array of zero-based indices I that identify which nodes belong to which graph. For instance: if node 8 belongs to the third graph, we will have I[8] == 2 . In the example above, color blue represents 0, green is 1, and orange is 2. In convolutional layers, disjoint mode is indistinguishable from single mode because it is not possible to exchange messages between the disjoint components of the graph, so I is not needed to compute the output. Pooling layers, on the other hand, require I to know which nodes can be pooled together. Let's load a dataset with many small graphs and have a look at the first three: >>> from spektral.datasets import TUDataset >>> dataset = TUDataset('PROTEINS') Successfully loaded PROTEINS. >>> dataset = dataset[:3] >>> dataset[0] Graph(n_nodes=42, n_node_features=4, n_edge_features=None, n_labels=2) >>> dataset[1] Graph(n_nodes=27, n_node_features=4, n_edge_features=None, n_labels=2) >>> dataset[2] Graph(n_nodes=10, n_node_features=4, n_edge_features=None, n_labels=2) To create batches in disjoint mode, we can use a DisjointLoader : >>> from spektral.data import DisjointLoader >>> loader = DisjointLoader(dataset, batch_size=3) Since Loaders are effectively generators, we can inspect the first batch by calling __next__() : >>> batch = loader.__next__() >>> inputs, target = batch >>> x, a, i = inputs >>> x.shape (79, 4) # 79 == 42 + 27 + 10 >>> a.shape (79, 79) >>> i.shape (79, ) Note that, since we don't have edge attributes in our dataset, the loader did not create the E matrix.","title":"Disjoint mode"},{"location":"data-modes/#batch-mode","text":"In batch mode, graphs are zero-padded so that they fit into tensors of shape [batch, N, ...] . Due to the general lack of support for sparse higher-order tensors both in Scipy and TensorFlow, X , A , and E will be dense tensors: A has shape [batch, nodes, nodes] ; X has shape [batch, nodes, n_feat] ; E has shape [batch, nodes, nodes, e_feat] (note that this is now the dense/ np.array format, in which the attributes of non-existing edges are all zeros). If the graphs have a variable number of nodes, nodes will be the size of the biggest graph in the batch. If you don't want to zero-pad the graphs or work with dense inputs, it is better to use disjoint mode instead. However, note that some pooling layers like DiffPool and MinCutPool will only work in batch mode. Let's re-use the dataset from the example above. We can use a BatchLoader as follows: >>> from spektral.data import BatchLoader >>> loader = BatchLoader(dataset, batch_size=3) >>> inputs, target = loader.__next__() >>> inputs[0].shape (3, 42, 4) >>> inputs[1].shape (3, 42, 42) In this case, the loader only created two inputs because we don't need the indices I . Also note that the batch was padded so that all graphs have 42 nodes, which is the size of the biggest graph out of the three. The BatchLoader zero-pads each batch independently of the others, so that we don't waste memory. If you want to remove the overhead of padding each batch, you can use a PackedBatchLoader which will pre-pad all graphs before yielding the batches. Of course, this means that all graphs will have the same number of nodes as the biggest graph in the dataset (and not just in the batch).","title":"Batch mode"},{"location":"data-modes/#mixed-mode","text":"In mixed mode we have a single graph that acts as the support for different node attributes (also sometimes called \"graph signals\"). In this case we have that: A is a matrix of shape [nodes, nodes] ; X is a tensor in batch mode, of shape [batch, nodes, n_feat] ; E has shape [batch, edges, e_feat] , where again we are representing each edge feature matrix E[b] , for b = 0 , ..., batch - 1 , in sparse format. Note that, since nodes and edges are the same for all graphs, we have stacked each x_i and e_i in higher-order tensors, similar to batch mode. An example of a mixed mode dataset is the MNIST random grid ( Defferrard et al., 2016 ): >>> from spektral.datasets import MNIST >>> dataset = MNIST() >>> dataset MNIST(n_graphs=70000) Mixed-mode datasets have a special a attribute that stores the adjacency matrix, while the proper graphs that make up the dataset only have node/edge features: >>>dataset.a <784x784 sparse matrix of type '<class 'numpy.float64'>' with 6396 stored elements in Compressed Sparse Row format> >>> dataset[0] Graph(n_nodes=784, n_node_features=1, n_edge_features=None, n_labels=1) >>>dataset[0].a # None We can use a MixedLoader to deal with sharing the adjacency matrix between the graphs in our dataset: >>> from spektral.data import MixedLoader >>> loader = MixedLoader(dataset, batch_size=3) >>> inputs, target = loader.__next__() >>> inputs[0].shape (3, 784, 1) >>> inputs[1].shape # Only one adjacency matrix (784, 784) Mixed mode requires a bit more work than the other three modes. In particular, it is not possible to use loader.load() to train a model in this mode. Have a look at this example to see how to train a GNN in mixed mode.","title":"Mixed mode"},{"location":"data/","text":"Data [source] Graph spektral.data.graph.Graph(x=None, a=None, e=None, y=None) A container to represent a graph. The data associated with the Graph is stored in its attributes: x , for the node features; a , for the adjacency matrix; e , for the edge attributes; y , for the node or graph labels; All of these default to None if you don't specify them in the constructor. If you want to read all non-None attributes at once, you can call the numpy() method, which will return all data in a tuple (with the order defined above). Graphs also have the following attributes that are computed automatically from the data: n_nodes : number of nodes; n_edges : number of edges; n_node_features : size of the node features, if available; n_edge_features : size of the edge features, if available; n_labels : size of the labels, if available; Any additional kwargs passed to the constructor will be automatically assigned as instance attributes of the graph. Data can be stored in Numpy arrays or Scipy sparse matrices, and labels can also be scalars. Spektral usually assumes that the different data matrices have specific shapes, although this is not strictly enforced to allow more flexibility. In general, node attributes should have shape (n_nodes, n_node_features) and the adjacency matrix should have shape (n_nodes, n_nodes) . Edge attributes can be stored in a dense format as arrays of shape (n_nodes, n_nodes, n_edge_features) or in a sparse format as arrays of shape (n_edges, n_edge_features) (so that you don't have to store all the zeros for missing edges). Most components of Spektral will know how to deal with both situations automatically. Labels can refer to the entire graph (shape (n_labels, ) ) or to each individual node (shape (n_nodes, n_labels) ). Arguments x : np.array, the node features (shape (n_nodes, n_node_features) ); a : np.array or scipy.sparse matrix, the adjacency matrix (shape (n_nodes, n_nodes) ); e : np.array, the edge features (shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ); y : np.array, the node or graph labels (shape (n_nodes, n_labels) or (n_labels, ) ); [source] Dataset spektral.data.dataset.Dataset(transforms=None) A container for Graph objects. This class can be extended to represent a graph dataset. To create a Dataset , you must implement the Dataset.read() method, which must return a list of spektral.data.Graph objects: class MyDataset(Dataset): def read(self): return [Graph(x=x, adj=adj, y=y) for x, adj, y in some_magic_list] The download() method is automatically called if the path returned by Dataset.path does not exists (default ~/.spektral/datasets/ClassName/ ). In this case, download() will be called before read() . Datasets should generally behave like Numpy arrays for any operation that uses simple 1D indexing: >>> dataset[0] Graph(...) >>> dataset[[1, 2, 3]] Dataset(n_graphs=3) >>> dataset[1:10] Dataset(n_graphs=9) >>> np.random.shuffle(dataset) # shuffle in-place >>> for graph in dataset[:3]: >>> print(graph) Graph(...) Graph(...) Graph(...) Datasets have the following properties that are automatically computed: n_nodes : the number of nodes in the dataset (always None, except in single and mixed mode datasets); n_node_features : the size of the node features (assumed to be equal for all graphs); n_edge_features : the size of the edge features (assumed to be equal for all graphs); n_labels : the size of the labels (assumed to be equal for all graphs); this is computed as y.shape[-1] . Any additional kwargs passed to the constructor will be automatically assigned as instance attributes of the dataset. Datasets also offer three main manipulation functions to apply callables to their graphs: apply(transform) : replaces each graph with the output of transform(graph) . See spektral.transforms for some ready-to-use transforms. Example: apply(spektral.transforms.NormalizeAdj()) normalizes the adjacency matrix of each graph in the dataset. map(transform, reduce=None) : returns a list containing the output of transform(graph) for each graph. If reduce is a callable , then returns reduce(output_list) . Example: map(lambda: g.n_nodes, reduce=np.mean) will return the average number of nodes in the dataset. filter(function) : removes from the dataset any graph for which function(graph) is False . Example: filter(lambda: g.n_nodes < 100) removes from the dataset all graphs bigger than 100 nodes. Datasets in mixed mode (one adjacency matrix, many instances of node features) are expected to have a particular structure. The graphs returned by read() should not have an adjacency matrix, which should be instead stored as a singleton in the dataset's a attribute. For example: class MyMixedModeDataset(Dataset): def read(self): self.a = compute_adjacency_matrix() return [Graph(x=x, y=y) for x, y in some_magic_list] Have a look at the spektral.datasets module for examples of popular datasets already implemented. Arguments transforms : a callable or list of callables that are automatically applied to the graphs after loading the dataset. Data utils to_disjoint spektral.data.utils.to_disjoint(x_list=None, a_list=None, e_list=None) Converts lists of node features, adjacency matrices and edge features to disjoint mode . Either the node features or the adjacency matrices must be provided as input. The i-th element of each list must be associated with the i-th graph. The method also computes the batch index to retrieve individual graphs from the disjoint union. Edge attributes can be represented as: a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as a stacked edge list. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes can change between graphs; a_list : a list of np.arrays or scipy.sparse matrices of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; Return Only if the corresponding list is given as input: x : np.array of shape (n_nodes, n_node_features) ; a : scipy.sparse matrix of shape (n_nodes, n_nodes) ; e : np.array of shape (n_edges, n_edge_features) ; i : np.array of shape (n_nodes, ) ; to_batch spektral.data.utils.to_batch(x_list=None, a_list=None, e_list=None, mask=False) Converts lists of node features, adjacency matrices and edge features to batch mode , by zero-padding all tensors to have the same node dimension n_max . Either the node features or the adjacency matrices must be provided as input. The i-th element of each list must be associated with the i-th graph. If a_list contains sparse matrices, they will be converted to dense np.arrays. The edge attributes of a graph can be represented as a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as dense arrays. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes can change between graphs; a_list : a list of np.arrays or scipy.sparse matrices of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; mask : bool, if True, node attributes will be extended with a binary mask that indicates valid nodes (the last feature of each node will be 1 if the node is valid and 0 otherwise). Use this flag in conjunction with layers.base.GraphMasking to start the propagation of masks in a model. Return Only if the corresponding list is given as input: x : np.array of shape (batch, n_max, n_node_features) ; a : np.array of shape (batch, n_max, n_max) ; e : np.array of shape (batch, n_max, n_max, n_edge_features) ; to_mixed spektral.data.utils.to_mixed(x_list=None, a=None, e_list=None) Converts lists of node features and edge features to mixed mode . The adjacency matrix must be passed as a singleton, i.e., a single np.array or scipy.sparse matrix shared by all graphs. Edge attributes can be represented as: a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as a batch of edge lists. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes must be the same between graphs; a : a np.array or scipy.sparse matrix of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; Return Only if the corresponding element is given as input: x : np.array of shape (batch, n_nodes, n_node_features) ; a : scipy.sparse matrix of shape (n_nodes, n_nodes) ; e : np.array of shape (batch, n_edges, n_edge_features) ; batch_generator spektral.data.utils.batch_generator(data, batch_size=32, epochs=None, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with the same first dimension; batch_size : number of samples in a batch; epochs : number of times to iterate over the data (default None, iterates indefinitely); shuffle : whether to shuffle the data at the beginning of each epoch Return Batches of size batch_size . to_tf_signature spektral.data.utils.to_tf_signature(signature) Converts a Dataset signature to a TensorFlow signature. Arguments signature : a Dataset signature. Return A TensorFlow signature.","title":"Containers"},{"location":"data/#data","text":"[source]","title":"Data"},{"location":"data/#graph","text":"spektral.data.graph.Graph(x=None, a=None, e=None, y=None) A container to represent a graph. The data associated with the Graph is stored in its attributes: x , for the node features; a , for the adjacency matrix; e , for the edge attributes; y , for the node or graph labels; All of these default to None if you don't specify them in the constructor. If you want to read all non-None attributes at once, you can call the numpy() method, which will return all data in a tuple (with the order defined above). Graphs also have the following attributes that are computed automatically from the data: n_nodes : number of nodes; n_edges : number of edges; n_node_features : size of the node features, if available; n_edge_features : size of the edge features, if available; n_labels : size of the labels, if available; Any additional kwargs passed to the constructor will be automatically assigned as instance attributes of the graph. Data can be stored in Numpy arrays or Scipy sparse matrices, and labels can also be scalars. Spektral usually assumes that the different data matrices have specific shapes, although this is not strictly enforced to allow more flexibility. In general, node attributes should have shape (n_nodes, n_node_features) and the adjacency matrix should have shape (n_nodes, n_nodes) . Edge attributes can be stored in a dense format as arrays of shape (n_nodes, n_nodes, n_edge_features) or in a sparse format as arrays of shape (n_edges, n_edge_features) (so that you don't have to store all the zeros for missing edges). Most components of Spektral will know how to deal with both situations automatically. Labels can refer to the entire graph (shape (n_labels, ) ) or to each individual node (shape (n_nodes, n_labels) ). Arguments x : np.array, the node features (shape (n_nodes, n_node_features) ); a : np.array or scipy.sparse matrix, the adjacency matrix (shape (n_nodes, n_nodes) ); e : np.array, the edge features (shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ); y : np.array, the node or graph labels (shape (n_nodes, n_labels) or (n_labels, ) ); [source]","title":"Graph"},{"location":"data/#dataset","text":"spektral.data.dataset.Dataset(transforms=None) A container for Graph objects. This class can be extended to represent a graph dataset. To create a Dataset , you must implement the Dataset.read() method, which must return a list of spektral.data.Graph objects: class MyDataset(Dataset): def read(self): return [Graph(x=x, adj=adj, y=y) for x, adj, y in some_magic_list] The download() method is automatically called if the path returned by Dataset.path does not exists (default ~/.spektral/datasets/ClassName/ ). In this case, download() will be called before read() . Datasets should generally behave like Numpy arrays for any operation that uses simple 1D indexing: >>> dataset[0] Graph(...) >>> dataset[[1, 2, 3]] Dataset(n_graphs=3) >>> dataset[1:10] Dataset(n_graphs=9) >>> np.random.shuffle(dataset) # shuffle in-place >>> for graph in dataset[:3]: >>> print(graph) Graph(...) Graph(...) Graph(...) Datasets have the following properties that are automatically computed: n_nodes : the number of nodes in the dataset (always None, except in single and mixed mode datasets); n_node_features : the size of the node features (assumed to be equal for all graphs); n_edge_features : the size of the edge features (assumed to be equal for all graphs); n_labels : the size of the labels (assumed to be equal for all graphs); this is computed as y.shape[-1] . Any additional kwargs passed to the constructor will be automatically assigned as instance attributes of the dataset. Datasets also offer three main manipulation functions to apply callables to their graphs: apply(transform) : replaces each graph with the output of transform(graph) . See spektral.transforms for some ready-to-use transforms. Example: apply(spektral.transforms.NormalizeAdj()) normalizes the adjacency matrix of each graph in the dataset. map(transform, reduce=None) : returns a list containing the output of transform(graph) for each graph. If reduce is a callable , then returns reduce(output_list) . Example: map(lambda: g.n_nodes, reduce=np.mean) will return the average number of nodes in the dataset. filter(function) : removes from the dataset any graph for which function(graph) is False . Example: filter(lambda: g.n_nodes < 100) removes from the dataset all graphs bigger than 100 nodes. Datasets in mixed mode (one adjacency matrix, many instances of node features) are expected to have a particular structure. The graphs returned by read() should not have an adjacency matrix, which should be instead stored as a singleton in the dataset's a attribute. For example: class MyMixedModeDataset(Dataset): def read(self): self.a = compute_adjacency_matrix() return [Graph(x=x, y=y) for x, y in some_magic_list] Have a look at the spektral.datasets module for examples of popular datasets already implemented. Arguments transforms : a callable or list of callables that are automatically applied to the graphs after loading the dataset.","title":"Dataset"},{"location":"data/#data-utils","text":"","title":"Data utils"},{"location":"data/#to_disjoint","text":"spektral.data.utils.to_disjoint(x_list=None, a_list=None, e_list=None) Converts lists of node features, adjacency matrices and edge features to disjoint mode . Either the node features or the adjacency matrices must be provided as input. The i-th element of each list must be associated with the i-th graph. The method also computes the batch index to retrieve individual graphs from the disjoint union. Edge attributes can be represented as: a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as a stacked edge list. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes can change between graphs; a_list : a list of np.arrays or scipy.sparse matrices of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; Return Only if the corresponding list is given as input: x : np.array of shape (n_nodes, n_node_features) ; a : scipy.sparse matrix of shape (n_nodes, n_nodes) ; e : np.array of shape (n_edges, n_edge_features) ; i : np.array of shape (n_nodes, ) ;","title":"to_disjoint"},{"location":"data/#to_batch","text":"spektral.data.utils.to_batch(x_list=None, a_list=None, e_list=None, mask=False) Converts lists of node features, adjacency matrices and edge features to batch mode , by zero-padding all tensors to have the same node dimension n_max . Either the node features or the adjacency matrices must be provided as input. The i-th element of each list must be associated with the i-th graph. If a_list contains sparse matrices, they will be converted to dense np.arrays. The edge attributes of a graph can be represented as a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as dense arrays. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes can change between graphs; a_list : a list of np.arrays or scipy.sparse matrices of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; mask : bool, if True, node attributes will be extended with a binary mask that indicates valid nodes (the last feature of each node will be 1 if the node is valid and 0 otherwise). Use this flag in conjunction with layers.base.GraphMasking to start the propagation of masks in a model. Return Only if the corresponding list is given as input: x : np.array of shape (batch, n_max, n_node_features) ; a : np.array of shape (batch, n_max, n_max) ; e : np.array of shape (batch, n_max, n_max, n_edge_features) ;","title":"to_batch"},{"location":"data/#to_mixed","text":"spektral.data.utils.to_mixed(x_list=None, a=None, e_list=None) Converts lists of node features and edge features to mixed mode . The adjacency matrix must be passed as a singleton, i.e., a single np.array or scipy.sparse matrix shared by all graphs. Edge attributes can be represented as: a dense array of shape (n_nodes, n_nodes, n_edge_features) ; a sparse edge list of shape (n_edges, n_edge_features) ; and they will always be returned as a batch of edge lists. Arguments x_list : a list of np.arrays of shape (n_nodes, n_node_features) -- note that n_nodes must be the same between graphs; a : a np.array or scipy.sparse matrix of shape (n_nodes, n_nodes) ; e_list : a list of np.arrays of shape (n_nodes, n_nodes, n_edge_features) or (n_edges, n_edge_features) ; Return Only if the corresponding element is given as input: x : np.array of shape (batch, n_nodes, n_node_features) ; a : scipy.sparse matrix of shape (n_nodes, n_nodes) ; e : np.array of shape (batch, n_edges, n_edge_features) ;","title":"to_mixed"},{"location":"data/#batch_generator","text":"spektral.data.utils.batch_generator(data, batch_size=32, epochs=None, shuffle=True) Iterates over the data for the given number of epochs, yielding batches of size batch_size . Arguments data : np.array or list of np.arrays with the same first dimension; batch_size : number of samples in a batch; epochs : number of times to iterate over the data (default None, iterates indefinitely); shuffle : whether to shuffle the data at the beginning of each epoch Return Batches of size batch_size .","title":"batch_generator"},{"location":"data/#to_tf_signature","text":"spektral.data.utils.to_tf_signature(signature) Converts a Dataset signature to a TensorFlow signature. Arguments signature : a Dataset signature. Return A TensorFlow signature.","title":"to_tf_signature"},{"location":"datasets/","text":"Datasets This module provides benchmark datasets for graph-level and node-level prediction. Datasets are automatically downloaded and saved locally on first usage. You can configure the path where the data are stored by creating a ~/.spektral/config.json file with the following content: { \"dataset_folder\": \"/path/to/dataset/folder\" } [source] Citation spektral.datasets.citation.Citation(name, random_split=False, normalize_x=False, dtype=<class 'numpy.float32'>) The citation datasets Cora, Citeseer and Pubmed. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the subject area of the paper. The train, test, and validation splits are given as binary masks and are accessible via the mask_tr , mask_va , and mask_te attributes. Arguments name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, as recommended by Shchur et al. (2018) ). If False (default), return the \"Planetoid\" public splits defined by Yang et al. (2016) . normalize_x : if True, normalize the features. dtype : numpy dtype of graph data. [source] GraphSage spektral.datasets.graphsage.GraphSage(name) The datasets used in the paper Inductive Representation Learning on Large Graphs William L. Hamilton et al. The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are given as binary masks and are accessible via the mask_tr , mask_va , and mask_te attributes. Arguments name : name of the dataset to load ( 'ppi' , or 'reddit' ); [source] PPI spektral.datasets.graphsage.PPI() Alias for GraphSage('ppi') . [source] Reddit spektral.datasets.graphsage.Reddit() Alias for GraphSage('reddit') . [source] MNIST spektral.datasets.mnist.MNIST(p_flip=0.0, k=8) The MNIST images used as node features for a grid graph, as described by Defferrard et al. (2016) . This dataset is a graph signal classification task, where graphs are represented in mixed mode: one adjacency matrix, many instances of node features. For efficiency, the adjacency matrix is stored in a special attribute of the dataset and the Graphs only contain the node features. You can access the adjacency matrix via the a attribute. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours on the grid. Labels represent the MNIST class associated to each sample. Note: the last 10000 samples are the default test set of the MNIST dataset. Arguments p_flip : if >0, then edges are randomly flipped from 0 to 1 or vice versa with that probability. k : number of neighbours of each node. [source] ModelNet spektral.datasets.modelnet.ModelNet(name, test=False, n_jobs=-1) The ModelNet10 and ModelNet40 CAD models datasets from the paper: 3D ShapeNets: A Deep Representation for Volumetric Shapes Zhirong Wu et al. Each graph represents a CAD model belonging to one of 10 (or 40) categories. The models are polygon meshes: the node attributes are the 3d coordinates of the vertices, and edges are computed from each face. Duplicate edges are ignored and the adjacency matrix is binary. The dataset are pre-split into training and test sets: the test flag controls which split is loaded. Arguments name : name of the dataset to load ('10' or '40'); test : if True, load the test set instead of the training set. n_jobs : number of CPU cores to use for reading the data (-1, to use all available cores) [source] OGB spektral.datasets.ogb.OGB(dataset) Wrapper for datasets from the Open Graph Benchmark (OGB) . Arguments dataset : an OGB library-agnostic dataset. [source] QM7 spektral.datasets.qm7.QM7() The QM7b dataset of molecules from the paper: MoleculeNet: A Benchmark for Molecular Machine Learning Zhenqin Wu et al. The dataset has no node features. Edges and edge features are obtained from the Coulomb matrices of the molecules. Each graph has a 14-dimensional label for regression. [source] QM9 spektral.datasets.qm9.QM9(amount=None, n_jobs=1) The QM9 chemical data set of small molecules. In this dataset, nodes represent atoms and edges represent chemical bonds. There are 5 possible atom types (H, C, N, O, F) and 4 bond types (single, double, triple, aromatic). Node features represent the chemical properties of each atom and include: The atomic number, one-hot encoded; The atom's position in the X, Y, and Z dimensions; The atomic charge; The mass difference from the monoisotope; The edge features represent the type of chemical bond between two atoms, one-hot encoded. Each graph has an 19-dimensional label for regression. Arguments amount : int, load this many molecules instead of the full dataset (useful for debugging). n_jobs : number of CPU cores to use for reading the data (-1, to use all available cores). [source] TUDataset spektral.datasets.tudataset.TUDataset(name, clean=False) The Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). Node features are computed by concatenating the following features for each node: node attributes, if available; node labels, if available, one-hot encoded. Some datasets might not have node features at all. In this case, attempting to use the dataset with a Loader will result in a crash. You can create node features using some of the transforms available in spektral.transforms or you can define your own features by accessing the individual samples in the graph attribute of the dataset (which is a list of Graph objects). Edge features are computed by concatenating the following features for each node: edge attributes, if available; edge labels, if available, one-hot encoded. Graph labels are provided for each dataset. Specific details about each individual dataset can be found in ~/.spektral/datasets/TUDataset/<dataset name>/README.md , after the dataset has been downloaded locally (datasets are downloaded automatically upon calling TUDataset('<dataset name>') the first time). Arguments name : str, name of the dataset to load (see TUD.available_datasets ). clean : if True , rload a version of the dataset with no isomorphic graphs.","title":"Datasets"},{"location":"datasets/#datasets","text":"This module provides benchmark datasets for graph-level and node-level prediction. Datasets are automatically downloaded and saved locally on first usage. You can configure the path where the data are stored by creating a ~/.spektral/config.json file with the following content: { \"dataset_folder\": \"/path/to/dataset/folder\" } [source]","title":"Datasets"},{"location":"datasets/#citation","text":"spektral.datasets.citation.Citation(name, random_split=False, normalize_x=False, dtype=<class 'numpy.float32'>) The citation datasets Cora, Citeseer and Pubmed. Node attributes are bag-of-words vectors representing the most common words in the text document associated to each node. Two papers are connected if either one cites the other. Labels represent the subject area of the paper. The train, test, and validation splits are given as binary masks and are accessible via the mask_tr , mask_va , and mask_te attributes. Arguments name : name of the dataset to load ( 'cora' , 'citeseer' , or 'pubmed' ); random_split : if True, return a randomized split (20 nodes per class for training, 30 nodes per class for validation and the remaining nodes for testing, as recommended by Shchur et al. (2018) ). If False (default), return the \"Planetoid\" public splits defined by Yang et al. (2016) . normalize_x : if True, normalize the features. dtype : numpy dtype of graph data. [source]","title":"Citation"},{"location":"datasets/#graphsage","text":"spektral.datasets.graphsage.GraphSage(name) The datasets used in the paper Inductive Representation Learning on Large Graphs William L. Hamilton et al. The PPI dataset (originally Stark et al. (2006) ) for inductive node classification uses positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels. The Reddit dataset consists of a graph made of Reddit posts in the month of September, 2014. The label for each node is the community that a post belongs to. The graph is built by sampling 50 large communities and two nodes are connected if the same user commented on both. Node features are obtained by concatenating the average GloVe CommonCrawl vectors of the title and comments, the post's score and the number of comments. The train, test, and validation splits are given as binary masks and are accessible via the mask_tr , mask_va , and mask_te attributes. Arguments name : name of the dataset to load ( 'ppi' , or 'reddit' ); [source]","title":"GraphSage"},{"location":"datasets/#ppi","text":"spektral.datasets.graphsage.PPI() Alias for GraphSage('ppi') . [source]","title":"PPI"},{"location":"datasets/#reddit","text":"spektral.datasets.graphsage.Reddit() Alias for GraphSage('reddit') . [source]","title":"Reddit"},{"location":"datasets/#mnist","text":"spektral.datasets.mnist.MNIST(p_flip=0.0, k=8) The MNIST images used as node features for a grid graph, as described by Defferrard et al. (2016) . This dataset is a graph signal classification task, where graphs are represented in mixed mode: one adjacency matrix, many instances of node features. For efficiency, the adjacency matrix is stored in a special attribute of the dataset and the Graphs only contain the node features. You can access the adjacency matrix via the a attribute. The node features of each graph are the MNIST digits vectorized and rescaled to [0, 1]. Two nodes are connected if they are neighbours on the grid. Labels represent the MNIST class associated to each sample. Note: the last 10000 samples are the default test set of the MNIST dataset. Arguments p_flip : if >0, then edges are randomly flipped from 0 to 1 or vice versa with that probability. k : number of neighbours of each node. [source]","title":"MNIST"},{"location":"datasets/#modelnet","text":"spektral.datasets.modelnet.ModelNet(name, test=False, n_jobs=-1) The ModelNet10 and ModelNet40 CAD models datasets from the paper: 3D ShapeNets: A Deep Representation for Volumetric Shapes Zhirong Wu et al. Each graph represents a CAD model belonging to one of 10 (or 40) categories. The models are polygon meshes: the node attributes are the 3d coordinates of the vertices, and edges are computed from each face. Duplicate edges are ignored and the adjacency matrix is binary. The dataset are pre-split into training and test sets: the test flag controls which split is loaded. Arguments name : name of the dataset to load ('10' or '40'); test : if True, load the test set instead of the training set. n_jobs : number of CPU cores to use for reading the data (-1, to use all available cores) [source]","title":"ModelNet"},{"location":"datasets/#ogb","text":"spektral.datasets.ogb.OGB(dataset) Wrapper for datasets from the Open Graph Benchmark (OGB) . Arguments dataset : an OGB library-agnostic dataset. [source]","title":"OGB"},{"location":"datasets/#qm7","text":"spektral.datasets.qm7.QM7() The QM7b dataset of molecules from the paper: MoleculeNet: A Benchmark for Molecular Machine Learning Zhenqin Wu et al. The dataset has no node features. Edges and edge features are obtained from the Coulomb matrices of the molecules. Each graph has a 14-dimensional label for regression. [source]","title":"QM7"},{"location":"datasets/#qm9","text":"spektral.datasets.qm9.QM9(amount=None, n_jobs=1) The QM9 chemical data set of small molecules. In this dataset, nodes represent atoms and edges represent chemical bonds. There are 5 possible atom types (H, C, N, O, F) and 4 bond types (single, double, triple, aromatic). Node features represent the chemical properties of each atom and include: The atomic number, one-hot encoded; The atom's position in the X, Y, and Z dimensions; The atomic charge; The mass difference from the monoisotope; The edge features represent the type of chemical bond between two atoms, one-hot encoded. Each graph has an 19-dimensional label for regression. Arguments amount : int, load this many molecules instead of the full dataset (useful for debugging). n_jobs : number of CPU cores to use for reading the data (-1, to use all available cores). [source]","title":"QM9"},{"location":"datasets/#tudataset","text":"spektral.datasets.tudataset.TUDataset(name, clean=False) The Benchmark Data Sets for Graph Kernels from TU Dortmund ( link ). Node features are computed by concatenating the following features for each node: node attributes, if available; node labels, if available, one-hot encoded. Some datasets might not have node features at all. In this case, attempting to use the dataset with a Loader will result in a crash. You can create node features using some of the transforms available in spektral.transforms or you can define your own features by accessing the individual samples in the graph attribute of the dataset (which is a list of Graph objects). Edge features are computed by concatenating the following features for each node: edge attributes, if available; edge labels, if available, one-hot encoded. Graph labels are provided for each dataset. Specific details about each individual dataset can be found in ~/.spektral/datasets/TUDataset/<dataset name>/README.md , after the dataset has been downloaded locally (datasets are downloaded automatically upon calling TUDataset('<dataset name>') the first time). Arguments name : str, name of the dataset to load (see TUD.available_datasets ). clean : if True , rload a version of the dataset with no isomorphic graphs.","title":"TUDataset"},{"location":"examples/","text":"Examples This is a collection of examples that you can use as template for your projects. Node-level prediction Citation networks with GCN Citation networks with GCN (custom training loop) Citation networks with ChebConv Citation networks with GAT Citation networks with GAT (custom training loop) Citation networks with ARMA Citation networks with SimpleGCN (custom transform) Open Graph Benchmark dataset Graph-level prediction General GNN Custom dataset OGB mol-hiv classification (edge attributes) QM9 regression with ECC (custom training loop) QM9 regression with ECC (batch mode) TUDataset classification with GIN TUDataset classification with MinCut pooling Other applications Graph signal classification on MNIST (mixed mode) Node clustering on citation networks with MinCut pooling (unsupervised)","title":"Examples"},{"location":"examples/#examples","text":"This is a collection of examples that you can use as template for your projects.","title":"Examples"},{"location":"examples/#node-level-prediction","text":"Citation networks with GCN Citation networks with GCN (custom training loop) Citation networks with ChebConv Citation networks with GAT Citation networks with GAT (custom training loop) Citation networks with ARMA Citation networks with SimpleGCN (custom transform) Open Graph Benchmark dataset","title":"Node-level prediction"},{"location":"examples/#graph-level-prediction","text":"General GNN Custom dataset OGB mol-hiv classification (edge attributes) QM9 regression with ECC (custom training loop) QM9 regression with ECC (batch mode) TUDataset classification with GIN TUDataset classification with MinCut pooling","title":"Graph-level prediction"},{"location":"examples/#other-applications","text":"Graph signal classification on MNIST (mixed mode) Node clustering on citation networks with MinCut pooling (unsupervised)","title":"Other applications"},{"location":"external/","text":"External resources This is a collection of additional material about Spektral. Paper We presented the library at the ICML 2020 workshop \"Graph Representation Learning and Beyond\". Paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi Notebooks The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"External resources"},{"location":"external/#external-resources","text":"This is a collection of additional material about Spektral.","title":"External resources"},{"location":"external/#paper","text":"We presented the library at the ICML 2020 workshop \"Graph Representation Learning and Beyond\". Paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi","title":"Paper"},{"location":"external/#notebooks","text":"The following notebooks are available on Kaggle with more visualizations (maintained by @kmader ): MNIST Graph Deep Learning ; MNIST Graph Pooling ;","title":"Notebooks"},{"location":"getting-started/","text":"Getting started Spektral is designed according to the guiding principles of Keras to make things extremely simple for beginners while maintaining flexibility for experts. In this tutorial, we will go over the main features of Spektral while creating a graph neural network for graph classification. Graphs A graph is a mathematical object that represents relations between entities. We call the entities \"nodes\" and the relations \"edges\". Both the nodes and the edges can have vector features . In Spektral, graphs are represented with instances of spektral.data.Graph . A graph can have four main attributes: a : the adjacency matrix x : the node features e : the edge features y : the labels A graph can have all of these attributes or none of them. Since Graphs are just plain Python objects, you can also add extra attributes if you want. For instance, see graph.n_nodes , graph.n_node_features , etc. Adjacency matrix ( graph.a ) Each entry a[i, j] of the adjacency matrix is non-zero if there exists an edge going from node i to node j , and zero otherwise. We can represent a as a dense np.array or as a Scipy sparse matrix of shape [n_nodes, n_nodes] . Using an np.array to represent the adjacency matrix can be expensive, since we need to store a lot of 0s in memory, so sparse matrices are usually preferable. With sparse matrices, we only need to store the non-zero entries of a . In practice, we can implement a sparse matrix by only storing the indices and values of the non-zero entries in a list, and assuming that if a pair of indices is missing from the list then its corresponding value will be 0. This is called the COOrdinate format and it is the format used by TensorFlow to represent sparse tensors. For example, the adjacency matrix of a weighted ring graph with 4 nodes: [[0, 1, 0, 2], [3, 0, 4, 0], [0, 5, 0, 6], [7, 0, 8, 0]] can be represented in COOrdinate format as follows: R, C, V 0, 1, 1 0, 3, 2 1, 0, 3 1, 2, 4 2, 1, 5 2, 3, 6 3, 0, 7 3, 2, 8 where R indicates the \"row\" indices, C the columns, and V the non-zero values a[i, j] . For example, in the second line, we see that there is an edge that goes from node 0 to node 3 with weight 2. We also see that, in this case, all edges have a corresponding edge that goes in the opposite direction. For the sake of this example, all edges have been assigned a different weight. In practice, however, edge i, j will often have the same weight as edge j, i and the adjacency matrix will be symmetric. Many convolutional and pooling layers in Spektral use this sparse representation of matrices to do their computation, and sometimes you will see in the documentation a comment saying that \"This layer expects a sparse adjacency matrix.\" Node features ( graph.x ) When working with graph neural networks, we usually associate a vector of features with each node of a graph. This is no different from how every pixel in an image has an [R, G, B, A] vector associated with it. Since we have n_nodes nodes and each node has a feature vector of size n_node_features , we can stack all features in a matrix x of shape [n_nodes, n_node_features] . In Spektral, x is always represented with a dense np.array (since in this case we don't run the risk of storing many useless zeros -- at least not often). Edge features ( graph.e ) Similar to node features, we can also have features associated with edges. These are usually different from the edge weights that we saw for the adjacency matrix, and often represent the kind of relation between two nodes (e.g., acquaintances, friends, or partners). When representing edge features, we run into the same problems that we have for the adjacency matrix. If we store them in a dense np.array , then the array will have shape [n_nodes, n_nodes, n_edge_features] and most of its entries will be zeros. Unfortunately, order-3 tensors cannot be represented as Scipy sparse matrices, so we need to be smart about it. Similar to how we stored the adjacency matrix as a list of entries r, c, v , here we can use the COOrdinate format to represent our edge features. Assume that, in the example above, each edge has n_edge_features=3 features. We could do something like: R, C, V 0, 1, [ef_1, ef_2, ef_3] 0, 3, [ef_1, ef_2, ef_3] 1, 0, [ef_1, ef_2, ef_3] 1, 2, [ef_1, ef_2, ef_3] 2, 1, [ef_1, ef_2, ef_3] 2, 3, [ef_1, ef_2, ef_3] 3, 0, [ef_1, ef_2, ef_3] 3, 2, [ef_1, ef_2, ef_3] Since we already have the information of R and C in the adjacency matrix, we only need to store the V column as a matrix e of shape [n_edges, n_edge_features] . In this case, n_edges indicates the number of non-zero entries in the adjacency matrix. Note that, since we have separated the edge features from the edge indices of the adjacency matrix, the order in which we store the edge features is very important. We must not break the correspondence between the edges in a and the edges in e . In Spektral, we always assume that edges are sorted in the row-major ordering (we first sort by row, then by column, like in the example above). This is not important when building the adjacency matrix, but it is important when building e . You can use spektral.utils.sparse.reorder to sort a matrix of edge features in the correct row-major order given by an edge index (i.e., the matrix obtained by stacking the R and C columns). Labels ( graph.y ) Finally, in many machine learning tasks we want to predict a label given an input. When working with GNNs, labels can be of two types: Graph labels represent some global properties of an entire graph; Node labels represent some properties of each individual node in a graph; Spektral supports both kinds. Labels are dense np.array s or scalars, stored in the y attribute of a Graph object. Graph-level labels can be either scalars or 1-dimensional arrays of shape [n_labels, ] . Node-level labels can be 1-dimensional arrays of shape [n_nodes, ] (representing a scalar label for each node), or 2-dimensional arrays of shape [n_nodes, n_labels] . This difference is relevant only when using a DisjointLoader ( read more here ). Datasets The spektral.data.Dataset container provides some useful functionality to manipulate collections of graphs. Let's load a popular benchmark dataset for graph classification: >>> from spektral.datasets import TUDataset >>> dataset = TUDataset('PROTEINS') >>> dataset TUDataset(n_graphs=1113) We can now retrieve individual graphs: >>> dataset[0] Graph(n_nodes=42, n_node_features=4, n_edge_features=None, y=[1. 0.]) or shuffle the data: >>> np.random.shuffle(dataset) or slice the dataset into sub-datsets: >>> dataset[:100] TUDataset(n_graphs=100) Datasets also provide methods for applying transforms to each datum: apply(transform) - modifies the dataset in-place, by applying the transform to each graph; map(transform) - returns a list obtained by applying the transform to each graph; filter(function) - removes from the dataset any graph for which function(graph) is False . This is also an in-place operation. For example, let's modify our dataset so that we only have graphs with less than 500 nodes: >>> dataset.filter(lambda g: g.n_nodes < 500) >>> dataset TUDataset(n_graphs=1111) # removed 2 graphs Now let's apply some transforms to our graphs. For example, we can modify each graph so that the node features also contain the one-hot-encoded degree of the nodes. First, we compute the maximum degree of the dataset, so that we know the size of the one-hot vectors: >>> max_degree = dataset.map(lambda g: g.a.sum(-1).max(), reduce=max) >>> max_degree 12 Try to go over the lambda function to see what it does. Also, notice that we passed a reduction function to the method, using the reduce keyword. This will be run on the output list computed by the map. Now we are ready to augment our node features with the one-hot-encoded degree. Spektral has a lot of pre-implemented transforms that we can use: >>> from spektral.transforms import Degree >>> dataset.apply(Degree(max_degree)) We can see that it worked because now we have an extra max_degree + 1 node features: >>> dataset[0] Graph(n_nodes=42, n_node_features=17, n_edge_features=None, y=[1. 0.]) Since we will be using a GCNConv layer in our GNN, we also want to follow the original paper that introduced this layer and do some extra pre-processing of the adjacency matrix. Since this is a fairly common operation, Spektral has a transform to do it: >>> from spektral.transforms import GCNFilter >>> dataset.apply(GCNFilter()) Many layers will require you to do some form of preprocessing. If you don't want to go back to the literature every time, every convolutional layer in Spektral has a preprocess(a) method that you can use to transform the adjacency matrix as needed. Have a look at the handy LayerPreprocess transform . Creating a GNN Creating GNNs is where Spektral really shines. Since Spektral is designed as an extension of Keras, you can plug any Spektral layer into a Keras Model without modifications. We just need to use the functional API because GNN layers usually need two or more inputs (so no Sequential models for now). For our first GNN, we will create a simple network that first does a bit of graph convolution, then sums all the nodes together (known as \"global pooling\"), and finally classifies the result with a dense softmax layer. We will also use dropout for regularization. Let's start by importing the necessary layers: from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense, Dropout from spektral.layers import GCNConv, GlobalSumPool Now we can use model subclassing to define our model: class MyFirstGNN(Model): def __init__(self, n_hidden, n_labels): super().__init__() self.graph_conv = GCNConv(n_hidden) self.pool = GlobalSumPool() self.dropout = Dropout(0.5) self.dense = Dense(n_labels, 'softmax') def call(self, inputs): out = self.graph_conv(inputs) out = self.dropout(out) out = self.pool(out) out = self.dense(out) return out And that's it! Note how we mixed layers from Spektral and Keras interchangeably: it's all just computation with tensors underneath. This also means that if you want to break free from Graph and Dataset and every other feature of Spektral, you can. Note: If you don't want to subclass Model to implement your GNN, you can also use the classical declarative style. You just need to pay attention to the Input and leave \"node\" dimensions unspecified (so None instead of n_nodes ). Training the GNN Now we're ready to train the GNN. First, we instantiate and compile our model: model = MyFirstGNN(32, dataset.n_labels) model.compile('adam', 'categorical_crossentropy') and we're almost there! However, here's where graphs get in our way. Unlike regular data, like images or sequences, graphs cannot be stretched, cut, or reshaped so that we can fit them into tensors of pre-defined shapes. If a graph has 10 nodes and another one has 4, we have to keep them that way. This means that iterating over a dataset in mini-batches is not trivial and we cannot simply use the model.fit() method of Keras as-is. We have to use a data Loader . Loaders Loaders iterate over a graph dataset to create mini-batches. They hide a lot of the complexity behind the process so that you don't need to think about it. You only need to go to this page and read up on data modes , so that you know which loader to use. Each loader has a load() method that returns a data generator that Keras can process. Since we're doing graph-level classification, we can use a BatchLoader . It's a bit slow and memory intensive (a DisjointLoader would have been better), but it lets us simplify the definition of MyFirstGNN . Again, go read about data modes after this tutorial. Let's create a data loader: from spektral.data import BatchLoader loader = BatchLoader(dataset_train, batch_size=32) and we can finally train our GNN! Since loaders are essentially generators, we need to provide the steps_per_epoch keyword to model.fit() and we don't need to specify a batch size: model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10) Done! Evaluating the GNN Evaluating the performance of our model, be it for testing or validation, follows a similar workflow. We create a data loader: from spektral.data import BatchLoader loader = BatchLoader(dataset_test, batch_size=32) and feed it to the model by calling load() : loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch) print('Test loss: {}'.format(loss)) Node-level learning Besides learning to predict labels for the whole graph, like in this tutorial, GNNs are very effective at learning to predict labels for each node. This is called \"node-level learning\" and we usually do it for datasets with one big graph (think a social network). For example, reproducing the results of the GCN paper for classifying nodes in a citation network can be done with GCNConv layers, the Citation dataset, and a SingleLoader : check out this example . As a matter of fact, check out all the examples . Go create! You are now ready to use Spektral to create your own GNNs. If you want to build a GNN for a specific task, chances are that everything you need is already in Spektral. Check out the examples for some ideas and practical tips. Remember to read the data modes section to learn about representing graphs and creating mini-batches. Make sure to read the documentation, and get in touch on Github if you have a feature that you want to see implemented. If you want to cite Spektral in your work, refer to our paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi","title":"Getting started"},{"location":"getting-started/#getting-started","text":"Spektral is designed according to the guiding principles of Keras to make things extremely simple for beginners while maintaining flexibility for experts. In this tutorial, we will go over the main features of Spektral while creating a graph neural network for graph classification.","title":"Getting started"},{"location":"getting-started/#graphs","text":"A graph is a mathematical object that represents relations between entities. We call the entities \"nodes\" and the relations \"edges\". Both the nodes and the edges can have vector features . In Spektral, graphs are represented with instances of spektral.data.Graph . A graph can have four main attributes: a : the adjacency matrix x : the node features e : the edge features y : the labels A graph can have all of these attributes or none of them. Since Graphs are just plain Python objects, you can also add extra attributes if you want. For instance, see graph.n_nodes , graph.n_node_features , etc.","title":"Graphs"},{"location":"getting-started/#adjacency-matrix-grapha","text":"Each entry a[i, j] of the adjacency matrix is non-zero if there exists an edge going from node i to node j , and zero otherwise. We can represent a as a dense np.array or as a Scipy sparse matrix of shape [n_nodes, n_nodes] . Using an np.array to represent the adjacency matrix can be expensive, since we need to store a lot of 0s in memory, so sparse matrices are usually preferable. With sparse matrices, we only need to store the non-zero entries of a . In practice, we can implement a sparse matrix by only storing the indices and values of the non-zero entries in a list, and assuming that if a pair of indices is missing from the list then its corresponding value will be 0. This is called the COOrdinate format and it is the format used by TensorFlow to represent sparse tensors. For example, the adjacency matrix of a weighted ring graph with 4 nodes: [[0, 1, 0, 2], [3, 0, 4, 0], [0, 5, 0, 6], [7, 0, 8, 0]] can be represented in COOrdinate format as follows: R, C, V 0, 1, 1 0, 3, 2 1, 0, 3 1, 2, 4 2, 1, 5 2, 3, 6 3, 0, 7 3, 2, 8 where R indicates the \"row\" indices, C the columns, and V the non-zero values a[i, j] . For example, in the second line, we see that there is an edge that goes from node 0 to node 3 with weight 2. We also see that, in this case, all edges have a corresponding edge that goes in the opposite direction. For the sake of this example, all edges have been assigned a different weight. In practice, however, edge i, j will often have the same weight as edge j, i and the adjacency matrix will be symmetric. Many convolutional and pooling layers in Spektral use this sparse representation of matrices to do their computation, and sometimes you will see in the documentation a comment saying that \"This layer expects a sparse adjacency matrix.\"","title":"Adjacency matrix (graph.a)"},{"location":"getting-started/#node-features-graphx","text":"When working with graph neural networks, we usually associate a vector of features with each node of a graph. This is no different from how every pixel in an image has an [R, G, B, A] vector associated with it. Since we have n_nodes nodes and each node has a feature vector of size n_node_features , we can stack all features in a matrix x of shape [n_nodes, n_node_features] . In Spektral, x is always represented with a dense np.array (since in this case we don't run the risk of storing many useless zeros -- at least not often).","title":"Node features (graph.x)"},{"location":"getting-started/#edge-features-graphe","text":"Similar to node features, we can also have features associated with edges. These are usually different from the edge weights that we saw for the adjacency matrix, and often represent the kind of relation between two nodes (e.g., acquaintances, friends, or partners). When representing edge features, we run into the same problems that we have for the adjacency matrix. If we store them in a dense np.array , then the array will have shape [n_nodes, n_nodes, n_edge_features] and most of its entries will be zeros. Unfortunately, order-3 tensors cannot be represented as Scipy sparse matrices, so we need to be smart about it. Similar to how we stored the adjacency matrix as a list of entries r, c, v , here we can use the COOrdinate format to represent our edge features. Assume that, in the example above, each edge has n_edge_features=3 features. We could do something like: R, C, V 0, 1, [ef_1, ef_2, ef_3] 0, 3, [ef_1, ef_2, ef_3] 1, 0, [ef_1, ef_2, ef_3] 1, 2, [ef_1, ef_2, ef_3] 2, 1, [ef_1, ef_2, ef_3] 2, 3, [ef_1, ef_2, ef_3] 3, 0, [ef_1, ef_2, ef_3] 3, 2, [ef_1, ef_2, ef_3] Since we already have the information of R and C in the adjacency matrix, we only need to store the V column as a matrix e of shape [n_edges, n_edge_features] . In this case, n_edges indicates the number of non-zero entries in the adjacency matrix. Note that, since we have separated the edge features from the edge indices of the adjacency matrix, the order in which we store the edge features is very important. We must not break the correspondence between the edges in a and the edges in e . In Spektral, we always assume that edges are sorted in the row-major ordering (we first sort by row, then by column, like in the example above). This is not important when building the adjacency matrix, but it is important when building e . You can use spektral.utils.sparse.reorder to sort a matrix of edge features in the correct row-major order given by an edge index (i.e., the matrix obtained by stacking the R and C columns).","title":"Edge features (graph.e)"},{"location":"getting-started/#labels-graphy","text":"Finally, in many machine learning tasks we want to predict a label given an input. When working with GNNs, labels can be of two types: Graph labels represent some global properties of an entire graph; Node labels represent some properties of each individual node in a graph; Spektral supports both kinds. Labels are dense np.array s or scalars, stored in the y attribute of a Graph object. Graph-level labels can be either scalars or 1-dimensional arrays of shape [n_labels, ] . Node-level labels can be 1-dimensional arrays of shape [n_nodes, ] (representing a scalar label for each node), or 2-dimensional arrays of shape [n_nodes, n_labels] . This difference is relevant only when using a DisjointLoader ( read more here ).","title":"Labels (graph.y)"},{"location":"getting-started/#datasets","text":"The spektral.data.Dataset container provides some useful functionality to manipulate collections of graphs. Let's load a popular benchmark dataset for graph classification: >>> from spektral.datasets import TUDataset >>> dataset = TUDataset('PROTEINS') >>> dataset TUDataset(n_graphs=1113) We can now retrieve individual graphs: >>> dataset[0] Graph(n_nodes=42, n_node_features=4, n_edge_features=None, y=[1. 0.]) or shuffle the data: >>> np.random.shuffle(dataset) or slice the dataset into sub-datsets: >>> dataset[:100] TUDataset(n_graphs=100) Datasets also provide methods for applying transforms to each datum: apply(transform) - modifies the dataset in-place, by applying the transform to each graph; map(transform) - returns a list obtained by applying the transform to each graph; filter(function) - removes from the dataset any graph for which function(graph) is False . This is also an in-place operation. For example, let's modify our dataset so that we only have graphs with less than 500 nodes: >>> dataset.filter(lambda g: g.n_nodes < 500) >>> dataset TUDataset(n_graphs=1111) # removed 2 graphs Now let's apply some transforms to our graphs. For example, we can modify each graph so that the node features also contain the one-hot-encoded degree of the nodes. First, we compute the maximum degree of the dataset, so that we know the size of the one-hot vectors: >>> max_degree = dataset.map(lambda g: g.a.sum(-1).max(), reduce=max) >>> max_degree 12 Try to go over the lambda function to see what it does. Also, notice that we passed a reduction function to the method, using the reduce keyword. This will be run on the output list computed by the map. Now we are ready to augment our node features with the one-hot-encoded degree. Spektral has a lot of pre-implemented transforms that we can use: >>> from spektral.transforms import Degree >>> dataset.apply(Degree(max_degree)) We can see that it worked because now we have an extra max_degree + 1 node features: >>> dataset[0] Graph(n_nodes=42, n_node_features=17, n_edge_features=None, y=[1. 0.]) Since we will be using a GCNConv layer in our GNN, we also want to follow the original paper that introduced this layer and do some extra pre-processing of the adjacency matrix. Since this is a fairly common operation, Spektral has a transform to do it: >>> from spektral.transforms import GCNFilter >>> dataset.apply(GCNFilter()) Many layers will require you to do some form of preprocessing. If you don't want to go back to the literature every time, every convolutional layer in Spektral has a preprocess(a) method that you can use to transform the adjacency matrix as needed. Have a look at the handy LayerPreprocess transform .","title":"Datasets"},{"location":"getting-started/#creating-a-gnn","text":"Creating GNNs is where Spektral really shines. Since Spektral is designed as an extension of Keras, you can plug any Spektral layer into a Keras Model without modifications. We just need to use the functional API because GNN layers usually need two or more inputs (so no Sequential models for now). For our first GNN, we will create a simple network that first does a bit of graph convolution, then sums all the nodes together (known as \"global pooling\"), and finally classifies the result with a dense softmax layer. We will also use dropout for regularization. Let's start by importing the necessary layers: from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense, Dropout from spektral.layers import GCNConv, GlobalSumPool Now we can use model subclassing to define our model: class MyFirstGNN(Model): def __init__(self, n_hidden, n_labels): super().__init__() self.graph_conv = GCNConv(n_hidden) self.pool = GlobalSumPool() self.dropout = Dropout(0.5) self.dense = Dense(n_labels, 'softmax') def call(self, inputs): out = self.graph_conv(inputs) out = self.dropout(out) out = self.pool(out) out = self.dense(out) return out And that's it! Note how we mixed layers from Spektral and Keras interchangeably: it's all just computation with tensors underneath. This also means that if you want to break free from Graph and Dataset and every other feature of Spektral, you can. Note: If you don't want to subclass Model to implement your GNN, you can also use the classical declarative style. You just need to pay attention to the Input and leave \"node\" dimensions unspecified (so None instead of n_nodes ).","title":"Creating a GNN"},{"location":"getting-started/#training-the-gnn","text":"Now we're ready to train the GNN. First, we instantiate and compile our model: model = MyFirstGNN(32, dataset.n_labels) model.compile('adam', 'categorical_crossentropy') and we're almost there! However, here's where graphs get in our way. Unlike regular data, like images or sequences, graphs cannot be stretched, cut, or reshaped so that we can fit them into tensors of pre-defined shapes. If a graph has 10 nodes and another one has 4, we have to keep them that way. This means that iterating over a dataset in mini-batches is not trivial and we cannot simply use the model.fit() method of Keras as-is. We have to use a data Loader .","title":"Training the GNN"},{"location":"getting-started/#loaders","text":"Loaders iterate over a graph dataset to create mini-batches. They hide a lot of the complexity behind the process so that you don't need to think about it. You only need to go to this page and read up on data modes , so that you know which loader to use. Each loader has a load() method that returns a data generator that Keras can process. Since we're doing graph-level classification, we can use a BatchLoader . It's a bit slow and memory intensive (a DisjointLoader would have been better), but it lets us simplify the definition of MyFirstGNN . Again, go read about data modes after this tutorial. Let's create a data loader: from spektral.data import BatchLoader loader = BatchLoader(dataset_train, batch_size=32) and we can finally train our GNN! Since loaders are essentially generators, we need to provide the steps_per_epoch keyword to model.fit() and we don't need to specify a batch size: model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=10) Done!","title":"Loaders"},{"location":"getting-started/#evaluating-the-gnn","text":"Evaluating the performance of our model, be it for testing or validation, follows a similar workflow. We create a data loader: from spektral.data import BatchLoader loader = BatchLoader(dataset_test, batch_size=32) and feed it to the model by calling load() : loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch) print('Test loss: {}'.format(loss))","title":"Evaluating the GNN"},{"location":"getting-started/#node-level-learning","text":"Besides learning to predict labels for the whole graph, like in this tutorial, GNNs are very effective at learning to predict labels for each node. This is called \"node-level learning\" and we usually do it for datasets with one big graph (think a social network). For example, reproducing the results of the GCN paper for classifying nodes in a citation network can be done with GCNConv layers, the Citation dataset, and a SingleLoader : check out this example . As a matter of fact, check out all the examples .","title":"Node-level learning"},{"location":"getting-started/#go-create","text":"You are now ready to use Spektral to create your own GNNs. If you want to build a GNN for a specific task, chances are that everything you need is already in Spektral. Check out the examples for some ideas and practical tips. Remember to read the data modes section to learn about representing graphs and creating mini-batches. Make sure to read the documentation, and get in touch on Github if you have a feature that you want to see implemented. If you want to cite Spektral in your work, refer to our paper: Graph Neural Networks in TensorFlow and Keras with Spektral Daniele Grattarola and Cesare Alippi","title":"Go create!"},{"location":"loaders/","text":"Loaders [source] Loader spektral.data.loaders.Loader(dataset, batch_size=1, epochs=None, shuffle=True) Parent class for data loaders. The role of a Loader is to iterate over a Dataset and yield batches of graphs to feed your Keras Models. This is achieved by having a generator object that produces lists of Graphs, which are then collated together and returned as Tensors. The core of a Loader is the collate(batch) method. This takes as input a list of Graph objects and returns a list of Tensors, np.arrays, or SparseTensors. For instance, if all graphs have the same number of nodes and size of the attributes, a simple collation function can be: def collate(self, batch): x = np.array([g.x for g in batch]) a = np.array([g.a for g in batch)] return x, a The load() method of a Loader returns an object that can be passed to a Keras model when using the fit , predict and evaluate functions. You can use it as follows: model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch) The steps_per_epoch property represents the number of batches that are in an epoch, and is a required keyword when calling fit , predict or evaluate with a Loader. If you are using a custom training function, you can specify the input signature of your batches with the tf.TypeSpec system to avoid unnecessary re-tracings. The signature is computed automatically by calling loader.tf_signature() . For example, a simple training step can be written as: @tf.function(input_signature=loader.tf_signature()) # Specify signature here def train_step(inputs, target): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_fn(target, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) We can then train our model in a loop as follows: for batch in loader: train_step(*batch) Arguments dataset : a spektral.data.Dataset object; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the dataset at the start of each epoch. [source] SingleLoader spektral.data.loaders.SingleLoader(dataset, epochs=None, sample_weights=None) A Loader for single mode . This loader produces Tensors representing a single graph. As such, it can only be used with Datasets of length 1 and the batch_size cannot be set. The loader supports sample weights through the sample_weights argument. If given, then each batch will be a tuple (inputs, labels, sample_weights) . Arguments dataset : a spektral.data.Dataset object with only one graph; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; sample_weights : Numpy array, will be appended to the output automatically. Output Returns a tuple (inputs, labels) or (inputs, labels, sample_weights) . inputs is a tuple containing the data matrices of the graph, only if they are not None : x : same as dataset[0].x ; a : same as dataset[0].a (scipy sparse matrices are converted to SparseTensors); e : same as dataset[0].e ; labels is the same as dataset[0].y . sample_weights is the same array passed when creating the loader. [source] DisjointLoader spektral.data.loaders.DisjointLoader(dataset, node_level=False, batch_size=1, epochs=None, shuffle=True) A Loader for disjoint mode . This loader represents a batch of graphs via their disjoint union. The loader automatically computes a batch index tensor, containing integer indices that map each node to its corresponding graph in the batch. The adjacency matrix os returned as a SparseTensor, regardless of the input. If node_level=False , the labels are interpreted as graph-level labels and are stacked along an additional dimension. If node_level=True , then the labels are stacked vertically. Note: TensorFlow 2.4 or above is required to use this Loader's load() method in a Keras training loop. Arguments dataset : a graph Dataset; node_level : bool, if True stack the labels vertically for node-level prediction; batch_size : size of the mini-batches; epochs : number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : whether to shuffle the data at the start of each epoch. Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [n_nodes, n_node_features] ; a : adjacency matrices of shape [n_nodes, n_nodes] ; e : edge attributes of shape [n_edges, n_edge_features] ; i : batch index of shape [n_nodes] . labels have shape [batch, n_labels] if node_level=False or [n_nodes, n_labels] otherwise. [source] BatchLoader spektral.data.loaders.BatchLoader(dataset, mask=False, batch_size=1, epochs=None, shuffle=True, node_level=False) A Loader for batch mode . This loader returns batches of graphs stacked along an extra dimension, with all \"node\" dimensions padded to be equal among all graphs. If n_max is the number of nodes of the biggest graph in the batch, then the padding consist of adding zeros to the node features, adjacency matrix, and edge attributes of each graph so that they have shapes [n_max, n_node_features] , [n_max, n_max] , and [n_max, n_max, n_edge_features] respectively. The zero-padding is done batch-wise, which saves up memory at the cost of more computation. If latency is an issue but memory isn't, or if the dataset has graphs with a similar number of nodes, you can use the PackedBatchLoader that zero-pads all the dataset once and then iterates over it. Note that the adjacency matrix and edge attributes are returned as dense arrays. if mask=True , node attributes will be extended with a binary mask that indicates valid nodes (the last feature of each node will be 1 if the node was originally in the graph and 0 if it is a fake node added by zero-padding). Use this flag in conjunction with layers.base.GraphMasking to start the propagation of masks in a model (necessary for node-level prediction and models that use a dense pooling layer like DiffPool or MinCutPool). If node_level=False , the labels are interpreted as graph-level labels and are returned as an array of shape [batch, n_labels] . If node_level=True , then the labels are padded along the node dimension and are returned as an array of shape [batch, n_max, n_labels] . Arguments dataset : a graph Dataset; mask : bool, whether to add a mask to the node features; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; node_level : bool, if True pad the labels along the node dimension; Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_max, n_node_features] ; a : adjacency matrices of shape [batch, n_max, n_max] ; e : edge attributes of shape [batch, n_max, n_max, n_edge_features] . labels have shape [batch, n_labels] if node_level=False or [batch, n_max, n_labels] otherwise. [source] PackedBatchLoader spektral.data.loaders.PackedBatchLoader(dataset, mask=False, batch_size=1, epochs=None, shuffle=True, node_level=False) A BatchLoader that zero-pads the graphs before iterating over the dataset. This means that n_max is computed over the whole dataset and not just a single batch. While using more memory than BatchLoader , this loader should reduce the computational overhead of padding each batch independently. Use this loader if: memory usage isn't an issue and you want to produce the batches as fast as possible; the graphs in the dataset have similar sizes and there are no outliers in the dataset (i.e., anomalous graphs with many more nodes than the dataset average). Arguments dataset : a graph Dataset; mask : bool, whether to add a mask to the node features; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; node_level : bool, if True pad the labels along the node dimension; Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_max, n_node_features] ; a : adjacency matrices of shape [batch, n_max, n_max] ; e : edge attributes of shape [batch, n_max, n_max, n_edge_features] . labels have shape [batch, n_labels] if node_level=False or [batch, n_max, n_labels] otherwise. [source] MixedLoader spektral.data.loaders.MixedLoader(dataset, batch_size=1, epochs=None, shuffle=True) A Loader for mixed mode . This loader returns batches where the node and edge attributes are stacked along an extra dimension, but the adjacency matrix is shared by all graphs. The loader expects all node and edge features to have the same number of nodes and edges. The dataset is pre-packed like in a PackedBatchLoader. Arguments dataset : a graph Dataset; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch. Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_nodes, n_node_features] ; a : adjacency matrix of shape [n_nodes, n_nodes] ; e : edge attributes of shape [batch, n_edges, n_edge_features] . labels have shape [batch, ..., n_labels] .","title":"Loaders"},{"location":"loaders/#loaders","text":"[source]","title":"Loaders"},{"location":"loaders/#loader","text":"spektral.data.loaders.Loader(dataset, batch_size=1, epochs=None, shuffle=True) Parent class for data loaders. The role of a Loader is to iterate over a Dataset and yield batches of graphs to feed your Keras Models. This is achieved by having a generator object that produces lists of Graphs, which are then collated together and returned as Tensors. The core of a Loader is the collate(batch) method. This takes as input a list of Graph objects and returns a list of Tensors, np.arrays, or SparseTensors. For instance, if all graphs have the same number of nodes and size of the attributes, a simple collation function can be: def collate(self, batch): x = np.array([g.x for g in batch]) a = np.array([g.a for g in batch)] return x, a The load() method of a Loader returns an object that can be passed to a Keras model when using the fit , predict and evaluate functions. You can use it as follows: model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch) The steps_per_epoch property represents the number of batches that are in an epoch, and is a required keyword when calling fit , predict or evaluate with a Loader. If you are using a custom training function, you can specify the input signature of your batches with the tf.TypeSpec system to avoid unnecessary re-tracings. The signature is computed automatically by calling loader.tf_signature() . For example, a simple training step can be written as: @tf.function(input_signature=loader.tf_signature()) # Specify signature here def train_step(inputs, target): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_fn(target, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) We can then train our model in a loop as follows: for batch in loader: train_step(*batch) Arguments dataset : a spektral.data.Dataset object; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the dataset at the start of each epoch. [source]","title":"Loader"},{"location":"loaders/#singleloader","text":"spektral.data.loaders.SingleLoader(dataset, epochs=None, sample_weights=None) A Loader for single mode . This loader produces Tensors representing a single graph. As such, it can only be used with Datasets of length 1 and the batch_size cannot be set. The loader supports sample weights through the sample_weights argument. If given, then each batch will be a tuple (inputs, labels, sample_weights) . Arguments dataset : a spektral.data.Dataset object with only one graph; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; sample_weights : Numpy array, will be appended to the output automatically. Output Returns a tuple (inputs, labels) or (inputs, labels, sample_weights) . inputs is a tuple containing the data matrices of the graph, only if they are not None : x : same as dataset[0].x ; a : same as dataset[0].a (scipy sparse matrices are converted to SparseTensors); e : same as dataset[0].e ; labels is the same as dataset[0].y . sample_weights is the same array passed when creating the loader. [source]","title":"SingleLoader"},{"location":"loaders/#disjointloader","text":"spektral.data.loaders.DisjointLoader(dataset, node_level=False, batch_size=1, epochs=None, shuffle=True) A Loader for disjoint mode . This loader represents a batch of graphs via their disjoint union. The loader automatically computes a batch index tensor, containing integer indices that map each node to its corresponding graph in the batch. The adjacency matrix os returned as a SparseTensor, regardless of the input. If node_level=False , the labels are interpreted as graph-level labels and are stacked along an additional dimension. If node_level=True , then the labels are stacked vertically. Note: TensorFlow 2.4 or above is required to use this Loader's load() method in a Keras training loop. Arguments dataset : a graph Dataset; node_level : bool, if True stack the labels vertically for node-level prediction; batch_size : size of the mini-batches; epochs : number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : whether to shuffle the data at the start of each epoch. Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [n_nodes, n_node_features] ; a : adjacency matrices of shape [n_nodes, n_nodes] ; e : edge attributes of shape [n_edges, n_edge_features] ; i : batch index of shape [n_nodes] . labels have shape [batch, n_labels] if node_level=False or [n_nodes, n_labels] otherwise. [source]","title":"DisjointLoader"},{"location":"loaders/#batchloader","text":"spektral.data.loaders.BatchLoader(dataset, mask=False, batch_size=1, epochs=None, shuffle=True, node_level=False) A Loader for batch mode . This loader returns batches of graphs stacked along an extra dimension, with all \"node\" dimensions padded to be equal among all graphs. If n_max is the number of nodes of the biggest graph in the batch, then the padding consist of adding zeros to the node features, adjacency matrix, and edge attributes of each graph so that they have shapes [n_max, n_node_features] , [n_max, n_max] , and [n_max, n_max, n_edge_features] respectively. The zero-padding is done batch-wise, which saves up memory at the cost of more computation. If latency is an issue but memory isn't, or if the dataset has graphs with a similar number of nodes, you can use the PackedBatchLoader that zero-pads all the dataset once and then iterates over it. Note that the adjacency matrix and edge attributes are returned as dense arrays. if mask=True , node attributes will be extended with a binary mask that indicates valid nodes (the last feature of each node will be 1 if the node was originally in the graph and 0 if it is a fake node added by zero-padding). Use this flag in conjunction with layers.base.GraphMasking to start the propagation of masks in a model (necessary for node-level prediction and models that use a dense pooling layer like DiffPool or MinCutPool). If node_level=False , the labels are interpreted as graph-level labels and are returned as an array of shape [batch, n_labels] . If node_level=True , then the labels are padded along the node dimension and are returned as an array of shape [batch, n_max, n_labels] . Arguments dataset : a graph Dataset; mask : bool, whether to add a mask to the node features; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; node_level : bool, if True pad the labels along the node dimension; Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_max, n_node_features] ; a : adjacency matrices of shape [batch, n_max, n_max] ; e : edge attributes of shape [batch, n_max, n_max, n_edge_features] . labels have shape [batch, n_labels] if node_level=False or [batch, n_max, n_labels] otherwise. [source]","title":"BatchLoader"},{"location":"loaders/#packedbatchloader","text":"spektral.data.loaders.PackedBatchLoader(dataset, mask=False, batch_size=1, epochs=None, shuffle=True, node_level=False) A BatchLoader that zero-pads the graphs before iterating over the dataset. This means that n_max is computed over the whole dataset and not just a single batch. While using more memory than BatchLoader , this loader should reduce the computational overhead of padding each batch independently. Use this loader if: memory usage isn't an issue and you want to produce the batches as fast as possible; the graphs in the dataset have similar sizes and there are no outliers in the dataset (i.e., anomalous graphs with many more nodes than the dataset average). Arguments dataset : a graph Dataset; mask : bool, whether to add a mask to the node features; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch; node_level : bool, if True pad the labels along the node dimension; Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_max, n_node_features] ; a : adjacency matrices of shape [batch, n_max, n_max] ; e : edge attributes of shape [batch, n_max, n_max, n_edge_features] . labels have shape [batch, n_labels] if node_level=False or [batch, n_max, n_labels] otherwise. [source]","title":"PackedBatchLoader"},{"location":"loaders/#mixedloader","text":"spektral.data.loaders.MixedLoader(dataset, batch_size=1, epochs=None, shuffle=True) A Loader for mixed mode . This loader returns batches where the node and edge attributes are stacked along an extra dimension, but the adjacency matrix is shared by all graphs. The loader expects all node and edge features to have the same number of nodes and edges. The dataset is pre-packed like in a PackedBatchLoader. Arguments dataset : a graph Dataset; batch_size : int, size of the mini-batches; epochs : int, number of epochs to iterate over the dataset. By default ( None ) iterates indefinitely; shuffle : bool, whether to shuffle the data at the start of each epoch. Output For each batch, returns a tuple (inputs, labels) . inputs is a tuple containing: x : node attributes of shape [batch, n_nodes, n_node_features] ; a : adjacency matrix of shape [n_nodes, n_nodes] ; e : edge attributes of shape [batch, n_edges, n_edge_features] . labels have shape [batch, ..., n_labels] .","title":"MixedLoader"},{"location":"models/","text":"Models This module implements ready-to-use models from recent literature. [source] GCN spektral.models.gcn.GCN(n_labels, channels=16, activation='relu', output_activation='softmax', use_bias=False, dropout_rate=0.5, l2_reg=0.00025) This model, with its default hyperparameters, implements the architecture from the paper: Semi-Supervised Classification with Graph Convolutional Networks Thomas N. Kipf and Max Welling Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) Weighted adjacency matrix of shape ([batch], n_nodes, n_nodes) Output Softmax predictions with shape ([batch], n_nodes, n_labels) . Arguments n_labels : number of channels in output; channels : number of channels in first GCNConv layer; activation : activation of the first GCNConv layer; output_activation : activation of the second GCNConv layer; use_bias : whether to add a learnable bias to the two GCNConv layers; dropout_rate : rate used in Dropout layers; l2_reg : l2 regularization strength; **kwargs : passed to Model.__init__ . [source] GeneralGNN spektral.models.general_gnn.GeneralGNN(output, activation=None, hidden=256, message_passing=4, pre_process=2, post_process=2, connectivity='cat', batch_norm=True, dropout=0.0, aggregate='sum', hidden_activation='prelu', pool='sum') This model implements the GNN architecture from the paper Design Space for Graph Neural Networks Jiaxuan You, Rex Ying, Jure Leskovec Mode : single, disjoint, mixed. The default parameters of the model are selected according to the best results obtained in the paper, and should provide a good performance on many node-level and graph-level tasks, without modifications. The defaults are as follows: 256 hidden channels 4 message passing layers 2 pre-processing layers 2 post-processing layers Skip connections with concatenation Batch normalization No dropout PReLU activations Sum aggregation in the message-passing layers Global sum pooling (not from the paper) The GNN uses the GeneralConv layer for message passing, and has a pre- and a post-processing MLP for the node features. Message-passing layers also have optional skip connections, which can be implemented as sum or concatenation. The dense layers of the pre-processing and post-processing MLPs compute the following update of the node features: \\h_i = \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_i \\W + \\b \\right) \\right) \\right) Message-passing layers compute: \\h_i = \\mathrm{Agg} \\left( \\left\\{ \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_j \\W + \\b \\right) \\right) \\right), j \\in \\mathcal{N}(i) \\right\\} \\right) Arguments output : int, the number of output units; activation : the activation function of the output layer. hidden : int, the number of hidden units for all layers except the output one; message_passing : int, the nummber of message-passing layers; pre_process : int, the number of layers in the pre-processing MLP; post_process : int, the number of layers in the post-processing MLP; connectivity : the type of skip connection. Can be: None, 'sum' or 'cat'; batch_norm : bool, whether to use batch normalization; dropout : float, dropout rate; aggregate : string or callable, an aggregation function. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. hidden_activation : activation function in the hidden layers. The PReLU activation can be used by passing hidden_activation='prelu' . pool : string or None, the global pooling function. If None, no global pooling is applied (e.g., for node-level learning). Supported pooling methods: 'sum', 'avg', 'max', 'attn', 'attn_sum', 'sort' (see spektral.layers.pooling.global_pool ). [source] GNNExplainer spektral.models.gnn_explainer.GNNExplainer(model, n_hops=None, preprocess=None, graph_level=False, verbose=False, learning_rate=0.01, a_size_coef=0.0005, x_size_coef=0.1, a_entropy_coef=0.1, x_entropy_coef=0.1, laplacian_coef=0.0) The GNNExplainer model from the paper: GNNExplainer: Generating Explanations for Graph Neural Networks Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik and Jure Leskovec. The model can be used to explain the predictions for a single node or for an entire graph. In both cases, it returns the subgraph that mostly contributes to the prediction. Arguments model : tf.keras.Model to explain; n_hops : number of hops from which the GNN aggregates info. If None , then the number is inferred from the Conv and MessagePassing layers in the model. preprocess : a preprocessing function to transform the adjacency matrix before giving it as input to the GNN; this is usually the same preprocess function of the Conv or MessagePassing layers used in the GNN (e.g., GCNConv.preprocess ). graph_level : if True, the GNN is assumed to be for graph-level prediction and the explanation is computed for the whole graph (and not just a node). verbose : if True, print info during training; learning_rate : learning rate when training the model; a_size_coef : coefficient to control the number of edges of the subgraph that contributes to the prediction; x_size_coef : coefficient to control the number of features of the subgraph that contributes to the prediction; a_entropy_coef : coefficient to control the discretization of the adjacency mask; x_entropy_coef : coefficient to control the discretization of the features mask; laplacian_coef : coefficient to control the graph Laplacian loss;","title":"Models"},{"location":"models/#models","text":"This module implements ready-to-use models from recent literature. [source]","title":"Models"},{"location":"models/#gcn","text":"spektral.models.gcn.GCN(n_labels, channels=16, activation='relu', output_activation='softmax', use_bias=False, dropout_rate=0.5, l2_reg=0.00025) This model, with its default hyperparameters, implements the architecture from the paper: Semi-Supervised Classification with Graph Convolutional Networks Thomas N. Kipf and Max Welling Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) Weighted adjacency matrix of shape ([batch], n_nodes, n_nodes) Output Softmax predictions with shape ([batch], n_nodes, n_labels) . Arguments n_labels : number of channels in output; channels : number of channels in first GCNConv layer; activation : activation of the first GCNConv layer; output_activation : activation of the second GCNConv layer; use_bias : whether to add a learnable bias to the two GCNConv layers; dropout_rate : rate used in Dropout layers; l2_reg : l2 regularization strength; **kwargs : passed to Model.__init__ . [source]","title":"GCN"},{"location":"models/#generalgnn","text":"spektral.models.general_gnn.GeneralGNN(output, activation=None, hidden=256, message_passing=4, pre_process=2, post_process=2, connectivity='cat', batch_norm=True, dropout=0.0, aggregate='sum', hidden_activation='prelu', pool='sum') This model implements the GNN architecture from the paper Design Space for Graph Neural Networks Jiaxuan You, Rex Ying, Jure Leskovec Mode : single, disjoint, mixed. The default parameters of the model are selected according to the best results obtained in the paper, and should provide a good performance on many node-level and graph-level tasks, without modifications. The defaults are as follows: 256 hidden channels 4 message passing layers 2 pre-processing layers 2 post-processing layers Skip connections with concatenation Batch normalization No dropout PReLU activations Sum aggregation in the message-passing layers Global sum pooling (not from the paper) The GNN uses the GeneralConv layer for message passing, and has a pre- and a post-processing MLP for the node features. Message-passing layers also have optional skip connections, which can be implemented as sum or concatenation. The dense layers of the pre-processing and post-processing MLPs compute the following update of the node features: \\h_i = \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_i \\W + \\b \\right) \\right) \\right) Message-passing layers compute: \\h_i = \\mathrm{Agg} \\left( \\left\\{ \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_j \\W + \\b \\right) \\right) \\right), j \\in \\mathcal{N}(i) \\right\\} \\right) Arguments output : int, the number of output units; activation : the activation function of the output layer. hidden : int, the number of hidden units for all layers except the output one; message_passing : int, the nummber of message-passing layers; pre_process : int, the number of layers in the pre-processing MLP; post_process : int, the number of layers in the post-processing MLP; connectivity : the type of skip connection. Can be: None, 'sum' or 'cat'; batch_norm : bool, whether to use batch normalization; dropout : float, dropout rate; aggregate : string or callable, an aggregation function. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. hidden_activation : activation function in the hidden layers. The PReLU activation can be used by passing hidden_activation='prelu' . pool : string or None, the global pooling function. If None, no global pooling is applied (e.g., for node-level learning). Supported pooling methods: 'sum', 'avg', 'max', 'attn', 'attn_sum', 'sort' (see spektral.layers.pooling.global_pool ). [source]","title":"GeneralGNN"},{"location":"models/#gnnexplainer","text":"spektral.models.gnn_explainer.GNNExplainer(model, n_hops=None, preprocess=None, graph_level=False, verbose=False, learning_rate=0.01, a_size_coef=0.0005, x_size_coef=0.1, a_entropy_coef=0.1, x_entropy_coef=0.1, laplacian_coef=0.0) The GNNExplainer model from the paper: GNNExplainer: Generating Explanations for Graph Neural Networks Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik and Jure Leskovec. The model can be used to explain the predictions for a single node or for an entire graph. In both cases, it returns the subgraph that mostly contributes to the prediction. Arguments model : tf.keras.Model to explain; n_hops : number of hops from which the GNN aggregates info. If None , then the number is inferred from the Conv and MessagePassing layers in the model. preprocess : a preprocessing function to transform the adjacency matrix before giving it as input to the GNN; this is usually the same preprocess function of the Conv or MessagePassing layers used in the GNN (e.g., GCNConv.preprocess ). graph_level : if True, the GNN is assumed to be for graph-level prediction and the explanation is computed for the whole graph (and not just a node). verbose : if True, print info during training; learning_rate : learning rate when training the model; a_size_coef : coefficient to control the number of edges of the subgraph that contributes to the prediction; x_size_coef : coefficient to control the number of features of the subgraph that contributes to the prediction; a_entropy_coef : coefficient to control the discretization of the adjacency mask; x_entropy_coef : coefficient to control the discretization of the features mask; laplacian_coef : coefficient to control the graph Laplacian loss;","title":"GNNExplainer"},{"location":"transforms/","text":"Transforms [source] AdjToSpTensor spektral.transforms.adj_to_sp_tensor.AdjToSpTensor() Converts the adjacency matrix to a SparseTensor. [source] ClusteringCoeff spektral.transforms.clustering_coefficient.ClusteringCoeff() Concatenates to each node attribute the clustering coefficient of the corresponding node. [source] Constant spektral.transforms.constant.Constant(value) Concatenates a constant value to the node attributes. Arguments value : the value to concatenate to the node attributes. [source] Delaunay spektral.transforms.delaunay.Delaunay() Computes the Delaunay triangulation of the node features. The adjacency matrix is obtained from the edges of the triangulation and replaces the previous adjacency matrix. Duplicate edges are ignored and the adjacency matrix is binary. Node features must be 2-dimensional. [source] Degree spektral.transforms.degree.Degree(max_degree) Concatenates to each node attribute the one-hot degree of the corresponding node. The adjacency matrix is expected to have integer entries and the degree is cast to integer before one-hot encoding. Arguments max_degree : the maximum degree of the nodes, i.e., the size of the one-hot vectors. [source] GCNFilter spektral.transforms.gcn_filter.GCNFilter(symmetric=True) Normalizes the adjacency matrix as described by Kipf & Welling (2017) : \\A \\leftarrow \\hat\\D^{-\\frac{1}{2}} (\\A + \\I) \\hat\\D^{-\\frac{1}{2}} where \\hat\\D_{ii} = 1 + \\sum\\limits_{j = 1}^{N} \\A_{ij} . Arguments symmetric : If False, then it computes \\hat\\D^{-1} (\\A + \\I) instead. [source] LayerPreprocess spektral.transforms.layer_preprocess.LayerPreprocess(layer_class) Applies the preprocess function of a convolutional Layer to the adjacency matrix. Arguments layer_class : the class of a layer from spektral.layers.convolutional , or any Layer that implements a preprocess(adj) method. [source] NormalizeAdj spektral.transforms.normalize_adj.NormalizeAdj(symmetric=True) Normalizes the adjacency matrix as: \\A \\leftarrow \\D^{-1/2}\\A\\D^{-1/2} Arguments symmetric : If False, then it computes \\D^{-1}\\A instead. [source] NormalizeOne spektral.transforms.normalize_one.NormalizeOne() Normalizes the node attributes by dividing each row by its sum, so that it sums to 1: \\X_i \\leftarrow \\frac{\\X_i}{\\sum_{j=1}^{N} \\X_{ij}} [source] NormalizeSphere spektral.transforms.normalize_sphere.NormalizeSphere() Normalizes the node attributes so that they are centered at the origin and contained within a sphere of radius 1: \\X_{i} \\leftarrow \\frac{\\X_{i} - \\bar\\X}{\\max_{i,j} \\X_{ij}} where \\bar\\X is the centroid of the node features. [source] OneHotLabels spektral.transforms.one_hot.OneHotLabels(depth=None, labels=None) One-hot encodes the graph labels along the innermost dimension (also if they are simple scalars). Either depth or labels must be passed as argument. Arguments depth : int, the size of the one-hot vector (labels are intended as indices for a vector of size depth ); labels : list or tuple, the possible values that the labels can take (labels are one-hot encoded according to where they are found in labels ).","title":"Transforms"},{"location":"transforms/#transforms","text":"[source]","title":"Transforms"},{"location":"transforms/#adjtosptensor","text":"spektral.transforms.adj_to_sp_tensor.AdjToSpTensor() Converts the adjacency matrix to a SparseTensor. [source]","title":"AdjToSpTensor"},{"location":"transforms/#clusteringcoeff","text":"spektral.transforms.clustering_coefficient.ClusteringCoeff() Concatenates to each node attribute the clustering coefficient of the corresponding node. [source]","title":"ClusteringCoeff"},{"location":"transforms/#constant","text":"spektral.transforms.constant.Constant(value) Concatenates a constant value to the node attributes. Arguments value : the value to concatenate to the node attributes. [source]","title":"Constant"},{"location":"transforms/#delaunay","text":"spektral.transforms.delaunay.Delaunay() Computes the Delaunay triangulation of the node features. The adjacency matrix is obtained from the edges of the triangulation and replaces the previous adjacency matrix. Duplicate edges are ignored and the adjacency matrix is binary. Node features must be 2-dimensional. [source]","title":"Delaunay"},{"location":"transforms/#degree","text":"spektral.transforms.degree.Degree(max_degree) Concatenates to each node attribute the one-hot degree of the corresponding node. The adjacency matrix is expected to have integer entries and the degree is cast to integer before one-hot encoding. Arguments max_degree : the maximum degree of the nodes, i.e., the size of the one-hot vectors. [source]","title":"Degree"},{"location":"transforms/#gcnfilter","text":"spektral.transforms.gcn_filter.GCNFilter(symmetric=True) Normalizes the adjacency matrix as described by Kipf & Welling (2017) : \\A \\leftarrow \\hat\\D^{-\\frac{1}{2}} (\\A + \\I) \\hat\\D^{-\\frac{1}{2}} where \\hat\\D_{ii} = 1 + \\sum\\limits_{j = 1}^{N} \\A_{ij} . Arguments symmetric : If False, then it computes \\hat\\D^{-1} (\\A + \\I) instead. [source]","title":"GCNFilter"},{"location":"transforms/#layerpreprocess","text":"spektral.transforms.layer_preprocess.LayerPreprocess(layer_class) Applies the preprocess function of a convolutional Layer to the adjacency matrix. Arguments layer_class : the class of a layer from spektral.layers.convolutional , or any Layer that implements a preprocess(adj) method. [source]","title":"LayerPreprocess"},{"location":"transforms/#normalizeadj","text":"spektral.transforms.normalize_adj.NormalizeAdj(symmetric=True) Normalizes the adjacency matrix as: \\A \\leftarrow \\D^{-1/2}\\A\\D^{-1/2} Arguments symmetric : If False, then it computes \\D^{-1}\\A instead. [source]","title":"NormalizeAdj"},{"location":"transforms/#normalizeone","text":"spektral.transforms.normalize_one.NormalizeOne() Normalizes the node attributes by dividing each row by its sum, so that it sums to 1: \\X_i \\leftarrow \\frac{\\X_i}{\\sum_{j=1}^{N} \\X_{ij}} [source]","title":"NormalizeOne"},{"location":"transforms/#normalizesphere","text":"spektral.transforms.normalize_sphere.NormalizeSphere() Normalizes the node attributes so that they are centered at the origin and contained within a sphere of radius 1: \\X_{i} \\leftarrow \\frac{\\X_{i} - \\bar\\X}{\\max_{i,j} \\X_{ij}} where \\bar\\X is the centroid of the node features. [source]","title":"NormalizeSphere"},{"location":"transforms/#onehotlabels","text":"spektral.transforms.one_hot.OneHotLabels(depth=None, labels=None) One-hot encodes the graph labels along the innermost dimension (also if they are simple scalars). Either depth or labels must be passed as argument. Arguments depth : int, the size of the one-hot vector (labels are intended as indices for a vector of size depth ); labels : list or tuple, the possible values that the labels can take (labels are one-hot encoded according to where they are found in labels ).","title":"OneHotLabels"},{"location":"layers/base/","text":"Base layers This module contains a miscellany of layers that are not specifically for graph neural networks. [source] Disjoint2Batch spektral.layers.Disjoint2Batch() Utility layer that converts data from disjoint mode to batch mode by zero-padding the node features and adjacency matrices. Mode : disjoint. This layer expects a sparse adjacency matrix. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) ; Graph IDs of shape (n_nodes, ) ; Output Batched node features of shape (batch, N_max, n_node_features) ; Batched adjacency matrix of shape (batch, N_max, N_max) ; [source] GraphMasking tensorflow.python.keras.engine.base_layer.GraphMasking() A layer that starts the propagation of masks in a model. This layer assumes that the node features given as input have been extended with a binary mask that indicates which nodes are valid in each graph. The layer is useful when using a data.BatchLoader with mask=True or in general when zero-padding graphs so that all batches have the same size. The binary mask indicates with a 1 those nodes that should be taken into account by the model. The layer will remove the rightmost feature from the nodes and start a mask propagation to all subsequent layers: print(x.shape) # shape (batch, n_nodes, n_node_features + 1) mask = x[..., -1:] # shape (batch, n_nodes, 1) x_new = x[..., :-1] # shape (batch, n_nodes, n_node_features) [source] InnerProduct spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (n_nodes, n_features) ; Output Tensor of shape (n_nodes, n_nodes) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source] MinkowskiProduct spektral.layers.MinkowskiProduct(activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (n_nodes, n_features) ; Output Tensor of shape (n_nodes, n_nodes) . Arguments activation : activation function;","title":"Base layers"},{"location":"layers/base/#base-layers","text":"This module contains a miscellany of layers that are not specifically for graph neural networks. [source]","title":"Base layers"},{"location":"layers/base/#disjoint2batch","text":"spektral.layers.Disjoint2Batch() Utility layer that converts data from disjoint mode to batch mode by zero-padding the node features and adjacency matrices. Mode : disjoint. This layer expects a sparse adjacency matrix. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) ; Graph IDs of shape (n_nodes, ) ; Output Batched node features of shape (batch, N_max, n_node_features) ; Batched adjacency matrix of shape (batch, N_max, N_max) ; [source]","title":"Disjoint2Batch"},{"location":"layers/base/#graphmasking","text":"tensorflow.python.keras.engine.base_layer.GraphMasking() A layer that starts the propagation of masks in a model. This layer assumes that the node features given as input have been extended with a binary mask that indicates which nodes are valid in each graph. The layer is useful when using a data.BatchLoader with mask=True or in general when zero-padding graphs so that all batches have the same size. The binary mask indicates with a 1 those nodes that should be taken into account by the model. The layer will remove the rightmost feature from the nodes and start a mask propagation to all subsequent layers: print(x.shape) # shape (batch, n_nodes, n_node_features + 1) mask = x[..., -1:] # shape (batch, n_nodes, 1) x_new = x[..., :-1] # shape (batch, n_nodes, n_node_features) [source]","title":"GraphMasking"},{"location":"layers/base/#innerproduct","text":"spektral.layers.InnerProduct(trainable_kernel=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) Computes the inner product between elements of a 2d Tensor: \\langle \\x, \\x \\rangle = \\x\\x^\\top. Mode : single. Input Tensor of shape (n_nodes, n_features) ; Output Tensor of shape (n_nodes, n_nodes) . Arguments trainable_kernel : add a trainable square matrix between the inner product (e.g., X @ W @ X.T ); activation : activation function; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the kernel; kernel_constraint : constraint applied to the kernel; [source]","title":"InnerProduct"},{"location":"layers/base/#minkowskiproduct","text":"spektral.layers.MinkowskiProduct(activation=None) Computes the hyperbolic inner product between elements of a rank 2 Tensor: \\langle \\x, \\x \\rangle = \\x \\, \\begin{pmatrix} \\I_{d \\times d} & 0 \\\\ 0 & -1 \\end{pmatrix} \\, \\x^\\top. Mode : single. Input Tensor of shape (n_nodes, n_features) ; Output Tensor of shape (n_nodes, n_nodes) . Arguments activation : activation function;","title":"MinkowskiProduct"},{"location":"layers/convolution/","text":"Convolutional layers The following convolutional/message-passing layers are available in Spektral. Notation: N : number of nodes; F : size of the node attributes; S : size of the edge attributes; \\x_i : node attributes of the i-th node; \\e_{i \\rightarrow j} : edge attributes of the edge from node i to node j; \\A : adjacency matrix; \\X : node attributes matrix; \\E : edge attributes matrix; \\D : degree matrix; \\W, \\V : trainable weights matrices; \\b : trainable bias vector; \\mathcal{N}(i) : one-hop neighbourhood of node i ; [source] MessagePassing spektral.layers.MessagePassing(aggregate='sum') A general class for message passing networks from the paper Neural Message Passing for Quantum Chemistry Justin Gilmer et al. Mode : single, disjoint. This layer and all of its extensions expect a sparse adjacency matrix. This layer computes: \\x_i' = \\gamma \\left( \\x_i, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi \\left(\\x_i, \\x_j, \\e_{j \\rightarrow i} \\right) \\right), where \\gamma is a differentiable update function, \\phi is a differentiable message function, \\square is a permutation-invariant function to aggregate the messages (like the sum or the average), and \\E_{ij} is the edge attribute of edge j-i. By extending this class, it is possible to create any message-passing layer in single/disjoint mode. API propagate(x, a, e=None, **kwargs) Propagates the messages and computes embeddings for each node in the graph. Any kwargs will be forwarded as keyword arguments to message() , aggregate() and update() . message(x, **kwargs) Computes messages, equivalent to \\phi in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. The get_sources and get_targets built-in methods can be used to automatically retrieve the node attributes of nodes that are sending (sources) or receiving (targets) a message. If you need direct access to the edge indices, you can use the index_sources and index_targets attributes. aggregate(messages, **kwargs) Aggregates the messages, equivalent to \\square in the definition. The behaviour of this function can also be controlled using the aggregate keyword in the constructor of the layer (supported aggregations: sum, mean, max, min, prod). Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. update(embeddings, **kwargs) Updates the aggregated messages to obtain the final node embeddings, equivalent to \\gamma in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Arguments : aggregate : string or callable, an aggregation function. This flag can be used to control the behaviour of aggregate() wihtout re-implementing it. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. If callable, the function must have the signature foo(updates, indices, n_nodes) and return a rank 2 tensor with shape (n_nodes, ...) . kwargs : additional keyword arguments specific to Keras' Layers, like regularizers, initializers, constraints, etc. [source] AGNNConv spektral.layers.AGNNConv(trainable=True, aggregate='sum', activation=None) An Attention-based Graph Neural Network (AGNN) from the paper Attention-based Graph Neural Network for Semi-supervised Learning Kiran K. Thekumparampil et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\X' = \\P\\X where \\P_{ij} = \\frac{ \\exp \\left( \\beta \\cos \\left( \\x_i, \\x_j \\right) \\right) }{ \\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp \\left( \\beta \\cos \\left( \\x_i, \\x_k \\right) \\right) } and \\beta is a trainable parameter. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input. Arguments trainable : boolean, if True, then beta is a trainable parameter. Otherwise, beta is fixed to 1; activation : activation function; [source] APPNPConv spektral.layers.APPNPConv(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) The APPNP operator from the paper Predict then Propagate: Graph Neural Networks meet Personalized PageRank Johannes Klicpera et al. Mode : single, disjoint, mixed, batch. This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability, \\textrm{MLP} is a multi-layer perceptron, and K is defined by the propagations argument. Input Node features of shape ([batch], n_nodes, n_node_features) ; Modified Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.gcn_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] ARMAConv spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An Auto-Regressive Moving Average convolutional layer (ARMA) from the paper Graph Neural Networks with convolutional ARMA filters Filippo Maria Bianchi et al. Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\A \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\A = \\D^{-1/2} \\A \\D^{-1/2}. Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized and rescaled Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] ChebConv spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer from the paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Micha\u00ebl Defferrard et al. Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I. Input Node features of shape ([batch], n_nodes, n_node_features) ; A list of K Chebyshev polynomials of shape [([batch], n_nodes, n_nodes), ..., ([batch], n_nodes, n_nodes)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] CrystalConv spektral.layers.CrystalConv(aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties Tian Xie and Jeffrey C. Grossman Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\x_i' = \\x_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\sigma \\left( \\z_{ij} \\W^{(f)} + \\b^{(f)} \\right) \\odot \\g \\left( \\z_{ij} \\W^{(s)} + \\b^{(s)} \\right) where \\z_{ij} = \\x_i \\| \\x_j \\| \\e_{ji} , \\sigma is a sigmoid activation, and g is the activation function (defined by the activation argument). Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Edge features of shape (num_edges, n_edge_features) . Output Node features with the same shape of the input. Arguments activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] DiffusionConv spektral.layers.DiffusionConv(channels, K=6, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A diffusion convolution operator from the paper Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting Yaguang Li et al. Mode : single, disjoint, mixed, batch. This layer expects a dense adjacency matrix. Given a number of diffusion steps K and a row-normalized adjacency matrix \\hat \\A , this layer calculates the q -th channel as: \\mathbf{X}_{~:,~q}' = \\sigma\\left( \\sum_{f=1}^{F} \\left( \\sum_{k=0}^{K-1} \\theta_k {\\hat \\A}^k \\right) \\X_{~:,~f} \\right) Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized adjacency or attention coef. matrix \\hat \\A of shape ([batch], n_nodes, n_nodes) ; Use DiffusionConvolution.preprocess to normalize. Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : number of diffusion steps. activation : activation function \\sigma ; ( \\tanh by default) kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] ECCConv spektral.layers.ECCConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) from the paper Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs Martin Simonovsky and Nikos Komodakis Mode : single, disjoint, batch, mixed. In single, disjoint, and mixed mode, this layer expects a sparse adjacency matrix. If a dense adjacency is given as input, it will be automatically cast to sparse, which might be expensive. This layer computes: \\x_i' = \\x_{i} \\W_{\\textrm{root}} + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\x_{j} \\textrm{MLP}(\\e_{j \\rightarrow i}) + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs an edge-specific weight as a function of edge attributes. Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrices of shape ([batch], n_nodes, n_nodes) ; Edge features. In single mode, shape (num_edges, n_edge_features) ; in batch mode, shape (batch, n_nodes, n_nodes, n_edge_features) . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers representing the hidden neurons of the kernel-generating network; 'root': if False, the layer will not consider the root node for computing the message passing (first term in equation above), but only the neighbours. activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] EdgeConv spektral.layers.EdgeConv(channels, mlp_hidden=None, mlp_activation='relu', aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge convolutional layer from the paper Dynamic Graph CNN for Learning on Point Clouds Yue Wang et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\x_i' = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}\\big( \\x_i \\| \\x_j - \\x_i \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GATConv spektral.layers.GATConv(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A Graph Attention layer (GAT) from the paper Graph Attention Networks Petar Veli\u010dkovi\u0107 et al. Mode : single, disjoint, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\X' = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} =\\frac{ \\exp\\left(\\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j]\\right)\\right)}{\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left(\\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k]\\right)\\right)} where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrix of shape ([batch], n_nodes, n_nodes) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], n_nodes, n_nodes) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one n_nodes x n_nodes matrix for each head). add_self_loops : if True, add self loops to the adjacency matrix. activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; attn_kernel_initializer : initializer for the attention weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source] GatedGraphConv spektral.layers.GatedGraphConv(channels, n_layers, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated graph convolutional layer from the paper Gated Graph Sequence Neural Networks Yujia Li et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes \\x_i' = \\h^{(L)}_i where: \\begin{align} & \\h^{(0)}_i = \\x_i \\| \\mathbf{0} \\\\ & \\m^{(l)}_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\h^{(l - 1)}_j \\W \\\\ & \\h^{(l)}_i = \\textrm{GRU} \\left(\\m^{(l)}_i, \\h^{(l - 1)}_i \\right) \\\\ \\end{align} where \\textrm{GRU} is a gated recurrent unit cell. Input Node features of shape (n_nodes, n_node_features) ; note that n_node_features must be smaller or equal than channels . Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; n_layers : integer, number of iterations with the GRU cell; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GCNConv spektral.layers.GCNConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) from the paper Semi-Supervised Classification with Graph Convolutional Networks Thomas N. Kipf and Max Welling Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], n_nodes, n_node_features) ; Modified Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.gcn_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GeneralConv spektral.layers.GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A general convolutional layer from the paper Design Space for Graph Neural Networks Jiaxuan You et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\x_i' = \\mathrm{Agg} \\left( \\left\\{ \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_j \\W + \\b \\right) \\right) \\right), j \\in \\mathcal{N}(i) \\right\\} \\right) where \\mathrm{Agg} is an aggregation function for the messages, \\mathrm{Act} is an activation function, \\mathrm{Dropout} applies dropout to the node features, and \\mathrm{BN} applies batch normalization to the node features. This layer supports the PReLU activation via the 'prelu' keyword. The default parameters of this layer are selected according to the best results obtained in the paper, and should provide a good performance on many node-level and graph-level tasks, without modifications. The defaults are as follows: 256 channels Batch normalization No dropout PReLU activation Sum aggregation If you are uncertain about which layers to use for your GNN, this is a safe choice. Check out the original paper for more specific configurations. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; batch_norm : bool, whether to use batch normalization; dropout : float, dropout rate; aggregate : string or callable, an aggregation function. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. activation : activation function. This layer also supports the advanced activation PReLU by passing activation='prelu' . use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GCSConv spektral.layers.GCSConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphConv layer with a trainable skip connection. Mode : single, disjoint, mixed, batch. This layer computes: \\Z' = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops. Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized adjacency matrix of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GINConv spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', mlp_batchnorm=True, aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) from the paper How Powerful are Graph Neural Networks? Keyulu Xu et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\x_i' = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\x_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\x_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see the original paper and the equation above. By setting epsilon=None , the parameter will be learned (default behaviour). If given as a value, the parameter will stay fixed. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; mlp_batchnorm : apply batch normalization after every hidden layer of the MLP; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] GraphSageConv spektral.layers.GraphSageConv(channels, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer from the paper Inductive Representation Learning on Large Graphs William L. Hamilton et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\X' = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\X' = \\frac{\\X'}{\\|\\X'\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] TAGConv spektral.layers.TAGConv(channels, K=3, aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Topology Adaptive Graph Convolutional layer (TAG) from the paper Topology Adaptive Graph Convolutional Networks Jian Du et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\sum\\limits_{k=0}^{K} \\D^{-1/2}\\A^k\\D^{-1/2}\\X\\W^{(k)} Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; K : the order of the layer (i.e., the layer will consider a K-hop neighbourhood for each node); activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source] XENetConv spektral.layers.XENetConv(stack_channels, node_channels, edge_channels, attention=True, node_activation=None, edge_activation=None, aggregate='sum', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A XENet convolutional layer from the paper XENet: Using a new graph convolution to accelerate the timeline for protein design on quantum computers Jack B. Maguire, Daniele Grattarola, Eugene Klyshko, Vikram Khipple Mulligan, Hans Melo Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. For a version of this layer that supports batch mode, you can use spektral.layers.XENetDenseConv as a drop-in replacement. This layer computes for each node i : \\s_{ij} = \\text{PReLU} \\left( (\\x_{i} \\| \\x_{j} \\| \\e_{ij} \\| \\e_{ji}) \\W^{(s)} + \\b^{(s)} \\right) \\\\ \\s^{(\\text{out})}_{i} = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\s_{ij} \\\\ \\s^{(\\text{in})}_{i} = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\s_{ji} \\\\ \\x_{i}' = \\sigma\\left( (\\x_{i} \\| \\s^{(\\text{out})}_{i} \\| \\s^{(\\text{in})}_{i}) \\W^{(n)} + \\b^{(n)} \\right) \\\\ \\e_{ij}' = \\sigma\\left( \\s_{ij} \\W^{(e)} + \\b^{(e)} \\right) Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrices of shape ([batch], n_nodes, n_nodes) ; Edge features of shape (num_edges, n_edge_features) ; Output Node features with the same shape of the input, but the last dimension changed to node_channels . Edge features with the same shape of the input, but the last dimension changed to edge_channels . Arguments stack_channels : integer or list of integers, number of channels for the hidden layers; node_channels : integer, number of output channels for the nodes; edge_channels : integer, number of output channels for the edges; attention : whether to use attention when aggregating the stacks; node_activation : activation function for nodes; edge_activation : activation function for edges; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector.","title":"Convolutional layers"},{"location":"layers/convolution/#convolutional-layers","text":"The following convolutional/message-passing layers are available in Spektral. Notation: N : number of nodes; F : size of the node attributes; S : size of the edge attributes; \\x_i : node attributes of the i-th node; \\e_{i \\rightarrow j} : edge attributes of the edge from node i to node j; \\A : adjacency matrix; \\X : node attributes matrix; \\E : edge attributes matrix; \\D : degree matrix; \\W, \\V : trainable weights matrices; \\b : trainable bias vector; \\mathcal{N}(i) : one-hop neighbourhood of node i ; [source]","title":"Convolutional layers"},{"location":"layers/convolution/#messagepassing","text":"spektral.layers.MessagePassing(aggregate='sum') A general class for message passing networks from the paper Neural Message Passing for Quantum Chemistry Justin Gilmer et al. Mode : single, disjoint. This layer and all of its extensions expect a sparse adjacency matrix. This layer computes: \\x_i' = \\gamma \\left( \\x_i, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi \\left(\\x_i, \\x_j, \\e_{j \\rightarrow i} \\right) \\right), where \\gamma is a differentiable update function, \\phi is a differentiable message function, \\square is a permutation-invariant function to aggregate the messages (like the sum or the average), and \\E_{ij} is the edge attribute of edge j-i. By extending this class, it is possible to create any message-passing layer in single/disjoint mode. API propagate(x, a, e=None, **kwargs) Propagates the messages and computes embeddings for each node in the graph. Any kwargs will be forwarded as keyword arguments to message() , aggregate() and update() . message(x, **kwargs) Computes messages, equivalent to \\phi in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. The get_sources and get_targets built-in methods can be used to automatically retrieve the node attributes of nodes that are sending (sources) or receiving (targets) a message. If you need direct access to the edge indices, you can use the index_sources and index_targets attributes. aggregate(messages, **kwargs) Aggregates the messages, equivalent to \\square in the definition. The behaviour of this function can also be controlled using the aggregate keyword in the constructor of the layer (supported aggregations: sum, mean, max, min, prod). Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. update(embeddings, **kwargs) Updates the aggregated messages to obtain the final node embeddings, equivalent to \\gamma in the definition. Any extra keyword argument of this function will be populated by propagate() if a matching keyword is found. Arguments : aggregate : string or callable, an aggregation function. This flag can be used to control the behaviour of aggregate() wihtout re-implementing it. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. If callable, the function must have the signature foo(updates, indices, n_nodes) and return a rank 2 tensor with shape (n_nodes, ...) . kwargs : additional keyword arguments specific to Keras' Layers, like regularizers, initializers, constraints, etc. [source]","title":"MessagePassing"},{"location":"layers/convolution/#agnnconv","text":"spektral.layers.AGNNConv(trainable=True, aggregate='sum', activation=None) An Attention-based Graph Neural Network (AGNN) from the paper Attention-based Graph Neural Network for Semi-supervised Learning Kiran K. Thekumparampil et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\X' = \\P\\X where \\P_{ij} = \\frac{ \\exp \\left( \\beta \\cos \\left( \\x_i, \\x_j \\right) \\right) }{ \\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp \\left( \\beta \\cos \\left( \\x_i, \\x_k \\right) \\right) } and \\beta is a trainable parameter. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input. Arguments trainable : boolean, if True, then beta is a trainable parameter. Otherwise, beta is fixed to 1; activation : activation function; [source]","title":"AGNNConv"},{"location":"layers/convolution/#appnpconv","text":"spektral.layers.APPNPConv(channels, alpha=0.2, propagations=1, mlp_hidden=None, mlp_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) The APPNP operator from the paper Predict then Propagate: Graph Neural Networks meet Personalized PageRank Johannes Klicpera et al. Mode : single, disjoint, mixed, batch. This layer computes: \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\ \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} + \\alpha \\Z^{(0)}, where \\alpha is the teleport probability, \\textrm{MLP} is a multi-layer perceptron, and K is defined by the propagations argument. Input Node features of shape ([batch], n_nodes, n_node_features) ; Modified Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.gcn_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; alpha : teleport probability during propagation; propagations : number of propagation steps; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; dropout_rate : dropout rate for Laplacian and MLP layers; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"APPNPConv"},{"location":"layers/convolution/#armaconv","text":"spektral.layers.ARMAConv(channels, order=1, iterations=1, share_weights=False, gcn_activation='relu', dropout_rate=0.0, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An Auto-Regressive Moving Average convolutional layer (ARMA) from the paper Graph Neural Networks with convolutional ARMA filters Filippo Maria Bianchi et al. Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)}, where K is the order of the ARMA _K filter, and where: \\bar \\X_k^{(t + 1)} = \\sigma \\left(\\tilde \\A \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right) is a recursive approximation of an ARMA _1 filter, where \\bar \\X^{(0)} = \\X and \\tilde \\A = \\D^{-1/2} \\A \\D^{-1/2}. Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized and rescaled Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.normalized_laplacian and spektral.utils.convolution.rescale_laplacian . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; order : order of the full ARMA _K filter, i.e., the number of parallel stacks in the layer; iterations : number of iterations to compute each ARMA _1 approximation; share_weights : share the weights in each ARMA _1 stack. gcn_activation : activation function to compute each ARMA _1 stack; dropout_rate : dropout rate for skip connection; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"ARMAConv"},{"location":"layers/convolution/#chebconv","text":"spektral.layers.ChebConv(channels, K=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Chebyshev convolutional layer from the paper Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Micha\u00ebl Defferrard et al. Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)} + \\b^{(k)}, where \\T^{(0)}, ..., \\T^{(K - 1)} are Chebyshev polynomials of \\tilde \\L defined as \\T^{(0)} = \\X \\\\ \\T^{(1)} = \\tilde \\L \\X \\\\ \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)}, where \\tilde \\L = \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I. Input Node features of shape ([batch], n_nodes, n_node_features) ; A list of K Chebyshev polynomials of shape [([batch], n_nodes, n_nodes), ..., ([batch], n_nodes, n_nodes)] ; can be computed with spektral.utils.convolution.chebyshev_filter . Output Node features with the same shape of the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : order of the Chebyshev polynomials; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"ChebConv"},{"location":"layers/convolution/#crystalconv","text":"spektral.layers.CrystalConv(aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A crystal graph convolutional layer from the paper Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties Tian Xie and Jeffrey C. Grossman Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\x_i' = \\x_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\sigma \\left( \\z_{ij} \\W^{(f)} + \\b^{(f)} \\right) \\odot \\g \\left( \\z_{ij} \\W^{(s)} + \\b^{(s)} \\right) where \\z_{ij} = \\x_i \\| \\x_j \\| \\e_{ji} , \\sigma is a sigmoid activation, and g is the activation function (defined by the activation argument). Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Edge features of shape (num_edges, n_edge_features) . Output Node features with the same shape of the input. Arguments activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"CrystalConv"},{"location":"layers/convolution/#diffusionconv","text":"spektral.layers.DiffusionConv(channels, K=6, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A diffusion convolution operator from the paper Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting Yaguang Li et al. Mode : single, disjoint, mixed, batch. This layer expects a dense adjacency matrix. Given a number of diffusion steps K and a row-normalized adjacency matrix \\hat \\A , this layer calculates the q -th channel as: \\mathbf{X}_{~:,~q}' = \\sigma\\left( \\sum_{f=1}^{F} \\left( \\sum_{k=0}^{K-1} \\theta_k {\\hat \\A}^k \\right) \\X_{~:,~f} \\right) Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized adjacency or attention coef. matrix \\hat \\A of shape ([batch], n_nodes, n_nodes) ; Use DiffusionConvolution.preprocess to normalize. Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; K : number of diffusion steps. activation : activation function \\sigma ; ( \\tanh by default) kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"DiffusionConv"},{"location":"layers/convolution/#eccconv","text":"spektral.layers.ECCConv(channels, kernel_network=None, root=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge-conditioned convolutional layer (ECC) from the paper Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs Martin Simonovsky and Nikos Komodakis Mode : single, disjoint, batch, mixed. In single, disjoint, and mixed mode, this layer expects a sparse adjacency matrix. If a dense adjacency is given as input, it will be automatically cast to sparse, which might be expensive. This layer computes: \\x_i' = \\x_{i} \\W_{\\textrm{root}} + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\x_{j} \\textrm{MLP}(\\e_{j \\rightarrow i}) + \\b where \\textrm{MLP} is a multi-layer perceptron that outputs an edge-specific weight as a function of edge attributes. Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrices of shape ([batch], n_nodes, n_nodes) ; Edge features. In single mode, shape (num_edges, n_edge_features) ; in batch mode, shape (batch, n_nodes, n_nodes, n_edge_features) . Output node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; kernel_network : a list of integers representing the hidden neurons of the kernel-generating network; 'root': if False, the layer will not consider the root node for computing the message passing (first term in equation above), but only the neighbours. activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"ECCConv"},{"location":"layers/convolution/#edgeconv","text":"spektral.layers.EdgeConv(channels, mlp_hidden=None, mlp_activation='relu', aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) An edge convolutional layer from the paper Dynamic Graph CNN for Learning on Point Clouds Yue Wang et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\x_i' = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}\\big( \\x_i \\| \\x_j - \\x_i \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"EdgeConv"},{"location":"layers/convolution/#gatconv","text":"spektral.layers.GATConv(channels, attn_heads=1, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None) A Graph Attention layer (GAT) from the paper Graph Attention Networks Petar Veli\u010dkovi\u0107 et al. Mode : single, disjoint, mixed, batch. This layer expects dense inputs when working in batch mode. This layer computes a convolution similar to layers.GraphConv , but uses the attention mechanism to weight the adjacency matrix instead of using the normalized Laplacian: \\X' = \\mathbf{\\alpha}\\X\\W + \\b where \\mathbf{\\alpha}_{ij} =\\frac{ \\exp\\left(\\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j]\\right)\\right)}{\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\exp\\left(\\mathrm{LeakyReLU}\\left( \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k]\\right)\\right)} where \\a \\in \\mathbb{R}^{2F'} is a trainable attention kernel. Dropout is also applied to \\alpha before computing \\Z . Parallel attention heads are computed in parallel and their results are aggregated by concatenation or average. Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrix of shape ([batch], n_nodes, n_nodes) ; Output Node features with the same shape as the input, but with the last dimension changed to channels ; if return_attn_coef=True , a list with the attention coefficients for each attention head. Each attention coefficient matrix has shape ([batch], n_nodes, n_nodes) . Arguments channels : number of output channels; attn_heads : number of attention heads to use; concat_heads : bool, whether to concatenate the output of the attention heads instead of averaging; dropout_rate : internal dropout rate for attention coefficients; return_attn_coef : if True, return the attention coefficients for the given input (one n_nodes x n_nodes matrix for each head). add_self_loops : if True, add self loops to the adjacency matrix. activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; attn_kernel_initializer : initializer for the attention weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; attn_kernel_regularizer : regularization applied to the attention kernels; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; attn_kernel_constraint : constraint applied to the attention kernels; bias_constraint : constraint applied to the bias vector. [source]","title":"GATConv"},{"location":"layers/convolution/#gatedgraphconv","text":"spektral.layers.GatedGraphConv(channels, n_layers, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated graph convolutional layer from the paper Gated Graph Sequence Neural Networks Yujia Li et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes \\x_i' = \\h^{(L)}_i where: \\begin{align} & \\h^{(0)}_i = \\x_i \\| \\mathbf{0} \\\\ & \\m^{(l)}_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\h^{(l - 1)}_j \\W \\\\ & \\h^{(l)}_i = \\textrm{GRU} \\left(\\m^{(l)}_i, \\h^{(l - 1)}_i \\right) \\\\ \\end{align} where \\textrm{GRU} is a gated recurrent unit cell. Input Node features of shape (n_nodes, n_node_features) ; note that n_node_features must be smaller or equal than channels . Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; n_layers : integer, number of iterations with the GRU cell; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GatedGraphConv"},{"location":"layers/convolution/#gcnconv","text":"spektral.layers.GCNConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A graph convolutional layer (GCN) from the paper Semi-Supervised Classification with Graph Convolutional Networks Thomas N. Kipf and Max Welling Mode : single, disjoint, mixed, batch. This layer computes: \\X' = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b where \\hat \\A = \\A + \\I is the adjacency matrix with added self-loops and \\hat\\D is its degree matrix. Input Node features of shape ([batch], n_nodes, n_node_features) ; Modified Laplacian of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.gcn_filter . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GCNConv"},{"location":"layers/convolution/#generalconv","text":"spektral.layers.GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A general convolutional layer from the paper Design Space for Graph Neural Networks Jiaxuan You et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\x_i' = \\mathrm{Agg} \\left( \\left\\{ \\mathrm{Act} \\left( \\mathrm{Dropout} \\left( \\mathrm{BN} \\left( \\x_j \\W + \\b \\right) \\right) \\right), j \\in \\mathcal{N}(i) \\right\\} \\right) where \\mathrm{Agg} is an aggregation function for the messages, \\mathrm{Act} is an activation function, \\mathrm{Dropout} applies dropout to the node features, and \\mathrm{BN} applies batch normalization to the node features. This layer supports the PReLU activation via the 'prelu' keyword. The default parameters of this layer are selected according to the best results obtained in the paper, and should provide a good performance on many node-level and graph-level tasks, without modifications. The defaults are as follows: 256 channels Batch normalization No dropout PReLU activation Sum aggregation If you are uncertain about which layers to use for your GNN, this is a safe choice. Check out the original paper for more specific configurations. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; batch_norm : bool, whether to use batch normalization; dropout : float, dropout rate; aggregate : string or callable, an aggregation function. Supported aggregations: 'sum', 'mean', 'max', 'min', 'prod'. activation : activation function. This layer also supports the advanced activation PReLU by passing activation='prelu' . use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GeneralConv"},{"location":"layers/convolution/#gcsconv","text":"spektral.layers.GCSConv(channels, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphConv layer with a trainable skip connection. Mode : single, disjoint, mixed, batch. This layer computes: \\Z' = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b where \\A does not have self-loops. Input Node features of shape ([batch], n_nodes, n_node_features) ; Normalized adjacency matrix of shape ([batch], n_nodes, n_nodes) ; can be computed with spektral.utils.convolution.normalized_adjacency . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GCSConv"},{"location":"layers/convolution/#ginconv","text":"spektral.layers.GINConv(channels, epsilon=None, mlp_hidden=None, mlp_activation='relu', mlp_batchnorm=True, aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Graph Isomorphism Network (GIN) from the paper How Powerful are Graph Neural Networks? Keyulu Xu et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes for each node i : \\x_i' = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\x_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\x_j \\big) where \\textrm{MLP} is a multi-layer perceptron. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; epsilon : unnamed parameter, see the original paper and the equation above. By setting epsilon=None , the parameter will be learned (default behaviour). If given as a value, the parameter will stay fixed. mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP (if None, the MLP has only the output layer); mlp_activation : activation for the MLP layers; mlp_batchnorm : apply batch normalization after every hidden layer of the MLP; activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GINConv"},{"location":"layers/convolution/#graphsageconv","text":"spektral.layers.GraphSageConv(channels, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A GraphSAGE layer from the paper Inductive Representation Learning on Large Graphs William L. Hamilton et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\X' = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\ \\X' = \\frac{\\X'}{\\|\\X'\\|} where \\textrm{AGGREGATE} is a function to aggregate a node's neighbourhood. The supported aggregation methods are: sum, mean, max, min, and product. Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape as the input, but with the last dimension changed to channels . Arguments channels : number of output channels; aggregate_op : str, aggregation method to use ( 'sum' , 'mean' , 'max' , 'min' , 'prod' ); activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"GraphSageConv"},{"location":"layers/convolution/#tagconv","text":"spektral.layers.TAGConv(channels, K=3, aggregate='sum', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A Topology Adaptive Graph Convolutional layer (TAG) from the paper Topology Adaptive Graph Convolutional Networks Jian Du et al. Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. This layer computes: \\Z = \\sum\\limits_{k=0}^{K} \\D^{-1/2}\\A^k\\D^{-1/2}\\X\\W^{(k)} Input Node features of shape (n_nodes, n_node_features) ; Binary adjacency matrix of shape (n_nodes, n_nodes) . Output Node features with the same shape of the input, but the last dimension changed to channels . Arguments channels : integer, number of output channels; K : the order of the layer (i.e., the layer will consider a K-hop neighbourhood for each node); activation : activation function; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector. [source]","title":"TAGConv"},{"location":"layers/convolution/#xenetconv","text":"spektral.layers.XENetConv(stack_channels, node_channels, edge_channels, attention=True, node_activation=None, edge_activation=None, aggregate='sum', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) A XENet convolutional layer from the paper XENet: Using a new graph convolution to accelerate the timeline for protein design on quantum computers Jack B. Maguire, Daniele Grattarola, Eugene Klyshko, Vikram Khipple Mulligan, Hans Melo Mode : single, disjoint, mixed. This layer expects a sparse adjacency matrix. For a version of this layer that supports batch mode, you can use spektral.layers.XENetDenseConv as a drop-in replacement. This layer computes for each node i : \\s_{ij} = \\text{PReLU} \\left( (\\x_{i} \\| \\x_{j} \\| \\e_{ij} \\| \\e_{ji}) \\W^{(s)} + \\b^{(s)} \\right) \\\\ \\s^{(\\text{out})}_{i} = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\s_{ij} \\\\ \\s^{(\\text{in})}_{i} = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\s_{ji} \\\\ \\x_{i}' = \\sigma\\left( (\\x_{i} \\| \\s^{(\\text{out})}_{i} \\| \\s^{(\\text{in})}_{i}) \\W^{(n)} + \\b^{(n)} \\right) \\\\ \\e_{ij}' = \\sigma\\left( \\s_{ij} \\W^{(e)} + \\b^{(e)} \\right) Input Node features of shape ([batch], n_nodes, n_node_features) ; Binary adjacency matrices of shape ([batch], n_nodes, n_nodes) ; Edge features of shape (num_edges, n_edge_features) ; Output Node features with the same shape of the input, but the last dimension changed to node_channels . Edge features with the same shape of the input, but the last dimension changed to edge_channels . Arguments stack_channels : integer or list of integers, number of channels for the hidden layers; node_channels : integer, number of output channels for the nodes; edge_channels : integer, number of output channels for the edges; attention : whether to use attention when aggregating the stacks; node_activation : activation function for nodes; edge_activation : activation function for edges; use_bias : bool, add a bias vector to the output; kernel_initializer : initializer for the weights; bias_initializer : initializer for the bias vector; kernel_regularizer : regularization applied to the weights; bias_regularizer : regularization applied to the bias vector; activity_regularizer : regularization applied to the output; kernel_constraint : constraint applied to the weights; bias_constraint : constraint applied to the bias vector.","title":"XENetConv"},{"location":"layers/pooling/","text":"Pooling layers The following pooling layers are available in Spektral. See the convolutional layers page for the notation. [source] SRCPool spektral.layers.SRCPool(return_selection=False) A general class for graph pooling layers based on the \"Select, Reduce, Connect\" framework presented in: Understanding Pooling in Graph Neural Networks. Daniele Grattarola et al. This layer computes: \\begin{align} & \\mathcal{S} = \\left\\{\\mathcal{S}_k\\right\\}_{k=1:K} = \\textsc{Sel}(\\mathcal{G}) \\\\ & \\mathcal{X}'=\\left\\{\\textsc{Red}( \\mathcal{G}, \\mathcal{S}_k )\\right\\}_{k=1:K} \\\\ & \\mathcal{E}'=\\left\\{\\textsc{Con}( \\mathcal{G}, \\mathcal{S}_k, \\mathcal{S}_l )\\right\\}_{k,L=1:K} \\\\ \\end{align} Where \\textsc{Sel} is a node equivariant selection function that computes the supernode assignments \\mathcal{S}_k , \\textsc{Red} is a permutation-invariant function to reduce the supernodes into the new node attributes, and \\textsc{Con} is a permutation-invariant connection function that computes the link between the pooled nodes. By extending this class, it is possible to create any pooling layer in the SRC formalism. Input x : Tensor of shape ([batch], N, F) representing node features; a : Tensor or SparseTensor of shape ([batch], N, N) representing the adjacency matrix; i : (optional) Tensor of integers with shape (N, ) representing the batch index; Output x_pool : Tensor of shape ([batch], K, F) , representing the node features of the output. K is the number of output nodes and depends on the specific pooling strategy; a_pool : Tensor or SparseTensor of shape ([batch], K, K) representing the adjacency matrix of the output; i_pool : (only if i was given as input) Tensor of integers with shape (K, ) representing the batch index of the output; s : (if return_selection=True ) Tensor or SparseTensor representing the supernode assignments; API pool(x, a, i, **kwargs) : pools the graph and returns the reduced node features and adjacency matrix. If the batch index i is not None , a reduced version of i will be returned as well. Any given kwargs will be passed as keyword arguments to select() , reduce() and connect() if any matching key is found. The mandatory arguments of pool() must be computed in call() by calling self.get_inputs(inputs) . select(x, a, i, **kwargs) : computes supernode assignments mapping the nodes of the input graph to the nodes of the output. reduce(x, s, **kwargs) : reduces the supernodes to form the nodes of the pooled graph. connect(a, s, **kwargs) : connects the reduced supernodes. reduce_index(i, s, **kwargs) : helper function to reduce the batch index (only called if i is given as input). When overriding any function of the API, it is possible to access the true number of nodes of the input ( n_nodes ) as a Tensor in the instance variable self.n_nodes (this is populated by self.get_inputs() at the beginning of call() ). Arguments : return_selection : if True , the Tensor used to represent supernode assignments will be returned with x_pool , a_pool , and i_pool ; [source] DiffPool spektral.layers.DiffPool(k, channels=None, return_selection=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer from the paper Hierarchical Graph Representation Learning with Differentiable Pooling Rex Ying et al. Mode : batch. This layer learns a soft clustering of the input graph as follows: \\begin{align} \\S &= \\textrm{GNN}_{embed}(\\A, \\X); \\\\ \\Z &= \\textrm{GNN}_{pool}(\\A, \\X); \\\\ \\X' &= \\S^\\top \\Z; \\\\ \\A' &= \\S^\\top \\A \\S; \\\\ \\end{align} where: \\textrm{GNN}_{\\square}(\\A, \\X) = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_{\\square}. The number of output channels of \\textrm{GNN}_{embed} is controlled by the channels parameter. Two auxiliary loss terms are also added to the model: the link prediction loss L_{LP} = \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss L_{E} - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer can be used without a supervised loss to compute node clustering by minimizing the two auxiliary losses. Input Node features of shape (batch, n_nodes_in, n_node_features) ; Adjacency matrix of shape (batch, n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (batch, n_nodes_out, channels) ; Reduced adjacency matrix of shape (batch, n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (batch, n_nodes_in, n_nodes_out) . Arguments k : number of output nodes; channels : number of output channels (if None , the number of output channels is the same as the input); return_selection : boolean, whether to return the selection matrix; activation : activation to apply after reduction; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] LaPool spektral.layers.LaPool(shortest_path_reg=True, return_selection=False) A Laplacian pooling (LaPool) layer from the paper Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling Emmanuel Noutahi et al. Mode : disjoint. This layer computes a soft clustering of the graph by first identifying a set of leaders, and then assigning every remaining node to the cluster of the closest leader: \\V = \\norm{\\L\\X}_d; \\\\ \\i = \\{ i \\mid \\V_i > \\V_j, \\forall j \\in \\cN(i) \\} \\\\ \\S^\\top = \\textrm{SparseMax}\\left( \\beta \\frac{\\X\\X_{\\i}^\\top}{\\norm{\\X}\\norm{\\X_{\\i}}} \\right) \\beta is a regularization vecotr that is applied element-wise to the selection matrix. If shortest_path_reg=True , it is equal to the inverse of the shortest path between each node and its corresponding leader (this can be expensive since it runs on CPU). Otherwise it is equal to 1. The reduction and connection are computed as \\X' = \\S\\X and \\A' = \\S^\\top\\A\\S , respectively. Note that the number of nodes in the output graph depends on the input node features. Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (n_nodes_out, channels) ; Reduced adjacency matrix of shape (n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (n_nodes_in, n_nodes_out) . Arguments shortest_path_reg : boolean, apply the shortest path regularization described in the papaer (can be expensive); return_selection : boolean, whether to return the selection matrix; [source] MinCutPool spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_selection=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A MinCut pooling layer from the paper Spectral Clustering with Graph Neural Networks for Graph Pooling Filippo Maria Bianchi et al. Mode : batch. This layer learns a soft clustering of the input graph as follows: \\begin{align} \\S &= \\textrm{MLP}(\\X); \\\\ \\X' &= \\S^\\top \\X \\\\ \\A' &= \\S^\\top \\A \\S; \\\\ \\end{align} where \\textrm{MLP} is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minimum cut loss L_c = - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss L_o = \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss to compute node clustering by minimizing the two auxiliary losses. Input Node features of shape (batch, n_nodes_in, n_node_features) ; Symmetrically normalized adjacency matrix of shape (batch, n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (batch, n_nodes_out, n_node_features) ; Reduced adjacency matrix of shape (batch, n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (batch, n_nodes_in, n_nodes_out) . Arguments k : number of output nodes; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None , the MLP has only one output layer); mlp_activation : activation for the MLP layers; return_selection : boolean, whether to return the selection matrix; use_bias : use bias in the MLP; kernel_initializer : initializer for the weights of the MLP; bias_initializer : initializer for the bias of the MLP; kernel_regularizer : regularization applied to the weights of the MLP; bias_regularizer : regularization applied to the bias of the MLP; kernel_constraint : constraint applied to the weights of the MLP; bias_constraint : constraint applied to the bias of the MLP; [source] SAGPool spektral.layers.SAGPool(ratio, return_selection=False, return_score=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer from the paper Self-Attention Graph Pooling Junhyun Lee et al. Mode : single, disjoint. This layer computes: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y and \\textrm{GNN}(\\A, \\X) = \\A \\X \\W. K is defined for each graph as a fraction of the number of nodes, controlled by the ratio argument. The gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Reduced node features of shape (ratio * n_nodes_in, n_node_features) ; Reduced adjacency matrix of shape (ratio * n_nodes_in, ratio * n_nodes_in) ; Reduced graph IDs of shape (ratio * n_nodes_in, ) (only in disjoint mode); If return_selection=True , the selection mask of shape (ratio * n_nodes_in, ) . If return_score=True , the scoring vector of shape (n_nodes_in, ) Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_selection : boolean, whether to return the selection mask; return_score : boolean, whether to return the node scoring vector; sigmoid_gating : boolean, use a sigmoid activation for gating instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source] TopKPool spektral.layers.TopKPool(ratio, return_selection=False, return_score=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer from the papers Graph U-Nets Hongyang Gao and Shuiwang Ji and Towards Sparse Hierarchical Graph Classifiers C\u0103t\u0103lina Cangea et al. Mode : single, disjoint. This layer computes: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . K is defined for each graph as a fraction of the number of nodes, controlled by the ratio argument. The gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Reduced node features of shape (ratio * n_nodes_in, n_node_features) ; Reduced adjacency matrix of shape (ratio * n_nodes_in, ratio * n_nodes_in) ; Reduced graph IDs of shape (ratio * n_nodes_in, ) (only in disjoint mode); If return_selection=True , the selection mask of shape (ratio * n_nodes_in, ) . If return_score=True , the scoring vector of shape (n_nodes_in, ) Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_selection : boolean, whether to return the selection mask; return_score : boolean, whether to return the node scoring vector; sigmoid_gating : boolean, use a sigmoid activation for gating instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; Global pooling layers [source] GlobalAvgPool spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source] GlobalMaxPool spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source] GlobalSumPool spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source] GlobalAttentionPool spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer from the paper Gated Graph Sequence Neural Networks Yujia Li et al. This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source] GlobalAttnSumPool spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments attn_kernel_initializer : initializer for the attention weights; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; [source] SortPool spektral.layers.SortPool(k) A SortPool layer as described by Zhang et al . This layers takes a graph signal \\mathbf{X} and returns the topmost k rows according to the last column. If \\mathbf{X} has less than k rows, the result is zero-padded to k. Mode : single, disjoint, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, k, n_node_features) (if single mode, shape will be (1, k, n_node_features) ). Arguments k : integer, number of nodes to keep;","title":"Pooling layers"},{"location":"layers/pooling/#pooling-layers","text":"The following pooling layers are available in Spektral. See the convolutional layers page for the notation. [source]","title":"Pooling layers"},{"location":"layers/pooling/#srcpool","text":"spektral.layers.SRCPool(return_selection=False) A general class for graph pooling layers based on the \"Select, Reduce, Connect\" framework presented in: Understanding Pooling in Graph Neural Networks. Daniele Grattarola et al. This layer computes: \\begin{align} & \\mathcal{S} = \\left\\{\\mathcal{S}_k\\right\\}_{k=1:K} = \\textsc{Sel}(\\mathcal{G}) \\\\ & \\mathcal{X}'=\\left\\{\\textsc{Red}( \\mathcal{G}, \\mathcal{S}_k )\\right\\}_{k=1:K} \\\\ & \\mathcal{E}'=\\left\\{\\textsc{Con}( \\mathcal{G}, \\mathcal{S}_k, \\mathcal{S}_l )\\right\\}_{k,L=1:K} \\\\ \\end{align} Where \\textsc{Sel} is a node equivariant selection function that computes the supernode assignments \\mathcal{S}_k , \\textsc{Red} is a permutation-invariant function to reduce the supernodes into the new node attributes, and \\textsc{Con} is a permutation-invariant connection function that computes the link between the pooled nodes. By extending this class, it is possible to create any pooling layer in the SRC formalism. Input x : Tensor of shape ([batch], N, F) representing node features; a : Tensor or SparseTensor of shape ([batch], N, N) representing the adjacency matrix; i : (optional) Tensor of integers with shape (N, ) representing the batch index; Output x_pool : Tensor of shape ([batch], K, F) , representing the node features of the output. K is the number of output nodes and depends on the specific pooling strategy; a_pool : Tensor or SparseTensor of shape ([batch], K, K) representing the adjacency matrix of the output; i_pool : (only if i was given as input) Tensor of integers with shape (K, ) representing the batch index of the output; s : (if return_selection=True ) Tensor or SparseTensor representing the supernode assignments; API pool(x, a, i, **kwargs) : pools the graph and returns the reduced node features and adjacency matrix. If the batch index i is not None , a reduced version of i will be returned as well. Any given kwargs will be passed as keyword arguments to select() , reduce() and connect() if any matching key is found. The mandatory arguments of pool() must be computed in call() by calling self.get_inputs(inputs) . select(x, a, i, **kwargs) : computes supernode assignments mapping the nodes of the input graph to the nodes of the output. reduce(x, s, **kwargs) : reduces the supernodes to form the nodes of the pooled graph. connect(a, s, **kwargs) : connects the reduced supernodes. reduce_index(i, s, **kwargs) : helper function to reduce the batch index (only called if i is given as input). When overriding any function of the API, it is possible to access the true number of nodes of the input ( n_nodes ) as a Tensor in the instance variable self.n_nodes (this is populated by self.get_inputs() at the beginning of call() ). Arguments : return_selection : if True , the Tensor used to represent supernode assignments will be returned with x_pool , a_pool , and i_pool ; [source]","title":"SRCPool"},{"location":"layers/pooling/#diffpool","text":"spektral.layers.DiffPool(k, channels=None, return_selection=False, activation=None, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A DiffPool layer from the paper Hierarchical Graph Representation Learning with Differentiable Pooling Rex Ying et al. Mode : batch. This layer learns a soft clustering of the input graph as follows: \\begin{align} \\S &= \\textrm{GNN}_{embed}(\\A, \\X); \\\\ \\Z &= \\textrm{GNN}_{pool}(\\A, \\X); \\\\ \\X' &= \\S^\\top \\Z; \\\\ \\A' &= \\S^\\top \\A \\S; \\\\ \\end{align} where: \\textrm{GNN}_{\\square}(\\A, \\X) = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_{\\square}. The number of output channels of \\textrm{GNN}_{embed} is controlled by the channels parameter. Two auxiliary loss terms are also added to the model: the link prediction loss L_{LP} = \\big\\| \\A - \\S\\S^\\top \\big\\|_F and the entropy loss L_{E} - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S). The layer can be used without a supervised loss to compute node clustering by minimizing the two auxiliary losses. Input Node features of shape (batch, n_nodes_in, n_node_features) ; Adjacency matrix of shape (batch, n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (batch, n_nodes_out, channels) ; Reduced adjacency matrix of shape (batch, n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (batch, n_nodes_in, n_nodes_out) . Arguments k : number of output nodes; channels : number of output channels (if None , the number of output channels is the same as the input); return_selection : boolean, whether to return the selection matrix; activation : activation to apply after reduction; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"DiffPool"},{"location":"layers/pooling/#lapool","text":"spektral.layers.LaPool(shortest_path_reg=True, return_selection=False) A Laplacian pooling (LaPool) layer from the paper Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling Emmanuel Noutahi et al. Mode : disjoint. This layer computes a soft clustering of the graph by first identifying a set of leaders, and then assigning every remaining node to the cluster of the closest leader: \\V = \\norm{\\L\\X}_d; \\\\ \\i = \\{ i \\mid \\V_i > \\V_j, \\forall j \\in \\cN(i) \\} \\\\ \\S^\\top = \\textrm{SparseMax}\\left( \\beta \\frac{\\X\\X_{\\i}^\\top}{\\norm{\\X}\\norm{\\X_{\\i}}} \\right) \\beta is a regularization vecotr that is applied element-wise to the selection matrix. If shortest_path_reg=True , it is equal to the inverse of the shortest path between each node and its corresponding leader (this can be expensive since it runs on CPU). Otherwise it is equal to 1. The reduction and connection are computed as \\X' = \\S\\X and \\A' = \\S^\\top\\A\\S , respectively. Note that the number of nodes in the output graph depends on the input node features. Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (n_nodes_out, channels) ; Reduced adjacency matrix of shape (n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (n_nodes_in, n_nodes_out) . Arguments shortest_path_reg : boolean, apply the shortest path regularization described in the papaer (can be expensive); return_selection : boolean, whether to return the selection matrix; [source]","title":"LaPool"},{"location":"layers/pooling/#mincutpool","text":"spektral.layers.MinCutPool(k, mlp_hidden=None, mlp_activation='relu', return_selection=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A MinCut pooling layer from the paper Spectral Clustering with Graph Neural Networks for Graph Pooling Filippo Maria Bianchi et al. Mode : batch. This layer learns a soft clustering of the input graph as follows: \\begin{align} \\S &= \\textrm{MLP}(\\X); \\\\ \\X' &= \\S^\\top \\X \\\\ \\A' &= \\S^\\top \\A \\S; \\\\ \\end{align} where \\textrm{MLP} is a multi-layer perceptron with softmax output. Two auxiliary loss terms are also added to the model: the minimum cut loss L_c = - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) } and the orthogonality loss L_o = \\left\\| \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F} - \\frac{\\I_K}{\\sqrt{K}} \\right\\|_F. The layer can be used without a supervised loss to compute node clustering by minimizing the two auxiliary losses. Input Node features of shape (batch, n_nodes_in, n_node_features) ; Symmetrically normalized adjacency matrix of shape (batch, n_nodes_in, n_nodes_in) ; Output Reduced node features of shape (batch, n_nodes_out, n_node_features) ; Reduced adjacency matrix of shape (batch, n_nodes_out, n_nodes_out) ; If return_selection=True , the selection matrix of shape (batch, n_nodes_in, n_nodes_out) . Arguments k : number of output nodes; mlp_hidden : list of integers, number of hidden units for each hidden layer in the MLP used to compute cluster assignments (if None , the MLP has only one output layer); mlp_activation : activation for the MLP layers; return_selection : boolean, whether to return the selection matrix; use_bias : use bias in the MLP; kernel_initializer : initializer for the weights of the MLP; bias_initializer : initializer for the bias of the MLP; kernel_regularizer : regularization applied to the weights of the MLP; bias_regularizer : regularization applied to the bias of the MLP; kernel_constraint : constraint applied to the weights of the MLP; bias_constraint : constraint applied to the bias of the MLP; [source]","title":"MinCutPool"},{"location":"layers/pooling/#sagpool","text":"spektral.layers.SAGPool(ratio, return_selection=False, return_score=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A self-attention graph pooling layer from the paper Self-Attention Graph Pooling Junhyun Lee et al. Mode : single, disjoint. This layer computes: \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y and \\textrm{GNN}(\\A, \\X) = \\A \\X \\W. K is defined for each graph as a fraction of the number of nodes, controlled by the ratio argument. The gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Reduced node features of shape (ratio * n_nodes_in, n_node_features) ; Reduced adjacency matrix of shape (ratio * n_nodes_in, ratio * n_nodes_in) ; Reduced graph IDs of shape (ratio * n_nodes_in, ) (only in disjoint mode); If return_selection=True , the selection mask of shape (ratio * n_nodes_in, ) . If return_score=True , the scoring vector of shape (n_nodes_in, ) Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_selection : boolean, whether to return the selection mask; return_score : boolean, whether to return the node scoring vector; sigmoid_gating : boolean, use a sigmoid activation for gating instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights; [source]","title":"SAGPool"},{"location":"layers/pooling/#topkpool","text":"spektral.layers.TopKPool(ratio, return_selection=False, return_score=False, sigmoid_gating=False, kernel_initializer='glorot_uniform', kernel_regularizer=None, kernel_constraint=None) A gPool/Top-K layer from the papers Graph U-Nets Hongyang Gao and Shuiwang Ji and Towards Sparse Hierarchical Graph Classifiers C\u0103t\u0103lina Cangea et al. Mode : single, disjoint. This layer computes: \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\; \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\; \\X' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\; \\A' = \\A_{\\i, \\i} where \\textrm{rank}(\\y, K) returns the indices of the top K values of \\y , and \\p is a learnable parameter vector of size F . K is defined for each graph as a fraction of the number of nodes, controlled by the ratio argument. The gating operation \\textrm{tanh}(\\y) (Cangea et al.) can be replaced with a sigmoid (Gao & Ji). Input Node features of shape (n_nodes_in, n_node_features) ; Adjacency matrix of shape (n_nodes_in, n_nodes_in) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Reduced node features of shape (ratio * n_nodes_in, n_node_features) ; Reduced adjacency matrix of shape (ratio * n_nodes_in, ratio * n_nodes_in) ; Reduced graph IDs of shape (ratio * n_nodes_in, ) (only in disjoint mode); If return_selection=True , the selection mask of shape (ratio * n_nodes_in, ) . If return_score=True , the scoring vector of shape (n_nodes_in, ) Arguments ratio : float between 0 and 1, ratio of nodes to keep in each graph; return_selection : boolean, whether to return the selection mask; return_score : boolean, whether to return the node scoring vector; sigmoid_gating : boolean, use a sigmoid activation for gating instead of a tanh; kernel_initializer : initializer for the weights; kernel_regularizer : regularization applied to the weights; kernel_constraint : constraint applied to the weights;","title":"TopKPool"},{"location":"layers/pooling/#global-pooling-layers","text":"[source]","title":"Global pooling layers"},{"location":"layers/pooling/#globalavgpool","text":"spektral.layers.GlobalAvgPool() An average pooling layer. Pools a graph by computing the average of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source]","title":"GlobalAvgPool"},{"location":"layers/pooling/#globalmaxpool","text":"spektral.layers.GlobalMaxPool() A max pooling layer. Pools a graph by computing the maximum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source]","title":"GlobalMaxPool"},{"location":"layers/pooling/#globalsumpool","text":"spektral.layers.GlobalSumPool() A global sum pooling layer. Pools a graph by computing the sum of its node features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments None. [source]","title":"GlobalSumPool"},{"location":"layers/pooling/#globalattentionpool","text":"spektral.layers.GlobalAttentionPool(channels, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None) A gated attention global pooling layer from the paper Gated Graph Sequence Neural Networks Yujia Li et al. This layer computes: \\X' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i where \\sigma is the sigmoid activation function. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, channels) (if single mode, shape will be (1, channels) ). Arguments channels : integer, number of output channels; bias_initializer : initializer for the bias vectors; kernel_regularizer : regularization applied to the kernel matrices; bias_regularizer : regularization applied to the bias vectors; kernel_constraint : constraint applied to the kernel matrices; bias_constraint : constraint applied to the bias vectors. [source]","title":"GlobalAttentionPool"},{"location":"layers/pooling/#globalattnsumpool","text":"spektral.layers.GlobalAttnSumPool(attn_kernel_initializer='glorot_uniform', attn_kernel_regularizer=None, attn_kernel_constraint=None) A node-attention global pooling layer. Pools a graph by learning attention coefficients to sum node features. This layer computes: \\alpha = \\textrm{softmax}( \\X \\a); \\\\ \\X' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i where \\a \\in \\mathbb{R}^F is a trainable vector. Note that the softmax is applied across nodes, and not across features. Mode : single, disjoint, mixed, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, n_node_features) (if single mode, shape will be (1, n_node_features) ). Arguments attn_kernel_initializer : initializer for the attention weights; attn_kernel_regularizer : regularization applied to the attention kernel matrix; attn_kernel_constraint : constraint applied to the attention kernel matrix; [source]","title":"GlobalAttnSumPool"},{"location":"layers/pooling/#sortpool","text":"spektral.layers.SortPool(k) A SortPool layer as described by Zhang et al . This layers takes a graph signal \\mathbf{X} and returns the topmost k rows according to the last column. If \\mathbf{X} has less than k rows, the result is zero-padded to k. Mode : single, disjoint, batch. Input Node features of shape ([batch], n_nodes, n_node_features) ; Graph IDs of shape (n_nodes, ) (only in disjoint mode); Output Pooled node features of shape (batch, k, n_node_features) (if single mode, shape will be (1, k, n_node_features) ). Arguments k : integer, number of nodes to keep;","title":"SortPool"},{"location":"utils/convolution/","text":"Convolution degree_matrix spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. degree_power spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format. normalized_adjacency spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix. laplacian spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian. normalized_laplacian spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian. rescale_laplacian spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eigenvalue when rescaling. Return add_self_loops spektral.utils.add_self_loops(a, value=1) Sets the inner diagonals of a to value . Arguments a : a np.array or scipy.sparse matrix, the innermost two dimensions must be equal. value : value to set the diagonals to. Return A np.array or scipy.sparse matrix with the same shape as a . gcn_filter spektral.utils.gcn_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A; chebyshev_polynomial spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial. chebyshev_filter spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"Convolution"},{"location":"utils/convolution/#convolution","text":"","title":"Convolution"},{"location":"utils/convolution/#degree_matrix","text":"spektral.utils.degree_matrix(A) Computes the degree matrix of the given adjacency matrix. Arguments A : rank 2 array or sparse matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_matrix"},{"location":"utils/convolution/#degree_power","text":"spektral.utils.degree_power(A, k) Computes \\D^{k} from the given adjacency matrix. Useful for computing normalised Laplacian. Arguments A : rank 2 array or sparse matrix. k : exponent to which elevate the degree matrix. Return If A is a dense array, a dense array; if A is sparse, a sparse matrix in DIA format.","title":"degree_power"},{"location":"utils/convolution/#normalized_adjacency","text":"spektral.utils.normalized_adjacency(A, symmetric=True) Normalizes the given adjacency matrix using the degree matrix as either \\D^{-1}\\A or \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized adjacency matrix.","title":"normalized_adjacency"},{"location":"utils/convolution/#laplacian","text":"spektral.utils.laplacian(A) Computes the Laplacian of the given adjacency matrix as \\D - \\A . Arguments A : rank 2 array or sparse matrix; Return The Laplacian.","title":"laplacian"},{"location":"utils/convolution/#normalized_laplacian","text":"spektral.utils.normalized_laplacian(A, symmetric=True) Computes a normalized Laplacian of the given adjacency matrix as \\I - \\D^{-1}\\A or \\I - \\D^{-1/2}\\A\\D^{-1/2} (symmetric normalization). Arguments A : rank 2 array or sparse matrix; symmetric : boolean, compute symmetric normalization; Return The normalized Laplacian.","title":"normalized_laplacian"},{"location":"utils/convolution/#rescale_laplacian","text":"spektral.utils.rescale_laplacian(L, lmax=None) Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue. Arguments L : rank 2 array or sparse matrix; lmax : if None, compute largest eigenvalue with scipy.linalg.eisgh. If the eigendecomposition fails, lmax is set to 2 automatically. If scalar, use this value as largest eigenvalue when rescaling. Return","title":"rescale_laplacian"},{"location":"utils/convolution/#add_self_loops","text":"spektral.utils.add_self_loops(a, value=1) Sets the inner diagonals of a to value . Arguments a : a np.array or scipy.sparse matrix, the innermost two dimensions must be equal. value : value to set the diagonals to. Return A np.array or scipy.sparse matrix with the same shape as a .","title":"add_self_loops"},{"location":"utils/convolution/#gcn_filter","text":"spektral.utils.gcn_filter(A, symmetric=True) Computes the graph filter described in Kipf & Welling (2017) . Arguments A : array or sparse matrix with rank 2 or 3; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return Array or sparse matrix with rank 2 or 3, same as A;","title":"gcn_filter"},{"location":"utils/convolution/#chebyshev_polynomial","text":"spektral.utils.chebyshev_polynomial(X, k) Calculates Chebyshev polynomials of X, up to order k. Arguments X : rank 2 array or sparse matrix; k : the order up to which compute the polynomials, Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_polynomial"},{"location":"utils/convolution/#chebyshev_filter","text":"spektral.utils.chebyshev_filter(A, k, symmetric=True) Computes the Chebyshev filter from the given adjacency matrix, as described in Defferrard et at. (2016) . Arguments A : rank 2 array or sparse matrix; k : integer, the order of the Chebyshev polynomial; symmetric : boolean, whether to normalize the matrix as \\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}} or as \\D^{-1}\\A ; Return A list of k + 1 arrays or sparse matrices with one element for each degree of the polynomial.","title":"chebyshev_filter"},{"location":"utils/misc/","text":"Miscellaneous pad_jagged_array spektral.utils.pad_jagged_array(x, target_shape) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays with variable dimensions; target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched. Return A np.array of shape (len(x), ) + target_shape . one_hot spektral.utils.one_hot(x, depth) One-hot encodes the integer array x in an array of length depth . Arguments x : a np.array of integers. depth : size of the one-hot vectors. Return An array of shape x.shape + (depth, ) label_to_one_hot spektral.utils.label_to_one_hot(x, labels) One-hot encodes the integer array x according to the given labels . Arguments x : a np.array of integers. Each value must be contained in labels . labels : list/tuple/np.array of labels. Return An array of shape x.shape + (len(labels), ) flatten_list spektral.utils.flatten_list(alist) Flattens an arbitrarily nested list to 1D. Arguments alist : a list or np.array (with at least one dimension), arbitrarily nested. Return A 1D Python list with the flattened elements as returned by a depth-first search.","title":"Miscellaneous"},{"location":"utils/misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"utils/misc/#pad_jagged_array","text":"spektral.utils.pad_jagged_array(x, target_shape) Given a jagged array of arbitrary dimensions, zero-pads all elements in the array to match the provided target_shape . Arguments x : a list or np.array of dtype object, containing np.arrays with variable dimensions; target_shape : a tuple or list s.t. target_shape[i] >= x.shape[i] for each x in X. If target_shape[i] = -1 , it will be automatically converted to X.shape[i], so that passing a target shape of e.g. (-1, n, m) will leave the first dimension of each element untouched. Return A np.array of shape (len(x), ) + target_shape .","title":"pad_jagged_array"},{"location":"utils/misc/#one_hot","text":"spektral.utils.one_hot(x, depth) One-hot encodes the integer array x in an array of length depth . Arguments x : a np.array of integers. depth : size of the one-hot vectors. Return An array of shape x.shape + (depth, )","title":"one_hot"},{"location":"utils/misc/#label_to_one_hot","text":"spektral.utils.label_to_one_hot(x, labels) One-hot encodes the integer array x according to the given labels . Arguments x : a np.array of integers. Each value must be contained in labels . labels : list/tuple/np.array of labels. Return An array of shape x.shape + (len(labels), )","title":"label_to_one_hot"},{"location":"utils/misc/#flatten_list","text":"spektral.utils.flatten_list(alist) Flattens an arbitrarily nested list to 1D. Arguments alist : a list or np.array (with at least one dimension), arbitrarily nested. Return A 1D Python list with the flattened elements as returned by a depth-first search.","title":"flatten_list"},{"location":"utils/sparse/","text":"Sparse reorder spektral.utils.reorder(edge_index, edge_weight=None, edge_features=None) Reorders edge_index , edge_weight , and edge_features according to the row-major ordering of edge_index . Arguments edge_index : np.array of shape [n_edges, 2] (representing rows and columns of the adjacency matrix), indices to sort in row-major order. edge_weight : np.array or None, edge weight to sort according to the new order of the indices. edge_features : np.array or None, edge features to sort according to the new order of the indices. Return edge_index: np.array, edge_index sorted in row-major order. edge_weight: If edge_weight is not None, edge_weight sorted according to the new order of the indices. Otherwise, None. edge_features: If edge_features is not None, edge_features sorted according to the new order of the indices. Otherwise, None. sp_matrix_to_sp_tensor spektral.utils.sp_matrix_to_sp_tensor(x) Converts a Scipy sparse matrix to a SparseTensor. The indices of the output are reordered in the canonical row-major ordering, and duplicate entries are summed together (which is the default behaviour of Scipy). Arguments x : a Scipy sparse matrix. Return A SparseTensor.","title":"Sparse"},{"location":"utils/sparse/#sparse","text":"","title":"Sparse"},{"location":"utils/sparse/#reorder","text":"spektral.utils.reorder(edge_index, edge_weight=None, edge_features=None) Reorders edge_index , edge_weight , and edge_features according to the row-major ordering of edge_index . Arguments edge_index : np.array of shape [n_edges, 2] (representing rows and columns of the adjacency matrix), indices to sort in row-major order. edge_weight : np.array or None, edge weight to sort according to the new order of the indices. edge_features : np.array or None, edge features to sort according to the new order of the indices. Return edge_index: np.array, edge_index sorted in row-major order. edge_weight: If edge_weight is not None, edge_weight sorted according to the new order of the indices. Otherwise, None. edge_features: If edge_features is not None, edge_features sorted according to the new order of the indices. Otherwise, None.","title":"reorder"},{"location":"utils/sparse/#sp_matrix_to_sp_tensor","text":"spektral.utils.sp_matrix_to_sp_tensor(x) Converts a Scipy sparse matrix to a SparseTensor. The indices of the output are reordered in the canonical row-major ordering, and duplicate entries are summed together (which is the default behaviour of Scipy). Arguments x : a Scipy sparse matrix. Return A SparseTensor.","title":"sp_matrix_to_sp_tensor"}]}